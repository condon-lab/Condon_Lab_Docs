{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Condon Research Group Docs This is a repository for holding all things documentation that might be helpful. It has been created using mkdocs . Contributing To contribute first you need to be added to the repo. Reach out to Laura and/or Will to be added. Then clone this repo git clone git@github.com:condon-lab/Condon_Lab_Docs.git Then you'll need to install the mkdocs package by running pip install mkdocs Checkout a new branch where you can add/edit docs git checkout -b <branch-name> Then edit docs to your heart's content! If you prefer to write documentation using google docs, you can convert a google doc to a markdown file easily using this chrome extension . Additionally, here are some helpful markdown tips and tricks . If you want to add an image to your documentation simply add the image you want to the ./images directory and then reference the image using the relative path. e.g. [alt text for your image](../../images/my_image.png) If you remove an image from an existing doc, please remove the image file also. You can then run mkdocs serve to see a local version of your new and updated docs in you browse at http://127.0.0.1:8000/ When everything looks good open a pull request to main and either ping in slack or assign Laura and/or Will to review. After your PR has been appoved, merge it in and go check out your new docs over at https://condon-lab.github.io/Condon_Lab_Docs/","title":"Condon Research Group Docs"},{"location":"#condon-research-group-docs","text":"This is a repository for holding all things documentation that might be helpful. It has been created using mkdocs .","title":"Condon Research Group Docs"},{"location":"#contributing","text":"To contribute first you need to be added to the repo. Reach out to Laura and/or Will to be added. Then clone this repo git clone git@github.com:condon-lab/Condon_Lab_Docs.git Then you'll need to install the mkdocs package by running pip install mkdocs Checkout a new branch where you can add/edit docs git checkout -b <branch-name> Then edit docs to your heart's content! If you prefer to write documentation using google docs, you can convert a google doc to a markdown file easily using this chrome extension . Additionally, here are some helpful markdown tips and tricks . If you want to add an image to your documentation simply add the image you want to the ./images directory and then reference the image using the relative path. e.g. [alt text for your image](../../images/my_image.png) If you remove an image from an existing doc, please remove the image file also. You can then run mkdocs serve to see a local version of your new and updated docs in you browse at http://127.0.0.1:8000/ When everything looks good open a pull request to main and either ping in slack or assign Laura and/or Will to review. After your PR has been appoved, merge it in and go check out your new docs over at https://condon-lab.github.io/Condon_Lab_Docs/","title":"Contributing"},{"location":"admin/pcard/","text":"Guidelines for using the PCARD (university credit card): Laura\u2019s lab group has a dedicated Pcard (credit card) as of 2024. Garry is the keeper of the card, please contact him about using the card for purchasing travel related items, technology and other work related items. The Pcard can be used to purchase airline tickets, pay conference registration fees and at times can be used for hotel expenses. The rules around what the Pcard can be used for changes occasionally, please check with Garry on the allowable expenses for Pcard use. If you are incorporating personal travel into travel for a conference or meeting, then the Pcard cannot be used. Many people choose not to use the Pcard for charges, since you will not get the miles or points associated with some personal credit cards.","title":"Pcard"},{"location":"admin/pcard/#guidelines-for-using-the-pcard-university-credit-card","text":"Laura\u2019s lab group has a dedicated Pcard (credit card) as of 2024. Garry is the keeper of the card, please contact him about using the card for purchasing travel related items, technology and other work related items. The Pcard can be used to purchase airline tickets, pay conference registration fees and at times can be used for hotel expenses. The rules around what the Pcard can be used for changes occasionally, please check with Garry on the allowable expenses for Pcard use. If you are incorporating personal travel into travel for a conference or meeting, then the Pcard cannot be used. Many people choose not to use the Pcard for charges, since you will not get the miles or points associated with some personal credit cards.","title":"Guidelines for using the PCARD (university credit card):"},{"location":"admin/travel/","text":"Traveling on University Business Traveling to conferences and other project related meetings is generally reimbursable but it is critical that you follow the following procedures. Please note that there are steps that need to happen before you travel so plan accordingly and make sure you follow all the steps in order to be reimbursed. If it\u2019s your first time traveling with UA you will need to first activate your account in the university travel system (see The first time you travel ). For all trips you need to do the following: Get approval from Laura for the travel you are planning on doing. Create a travel authorization and travel request using the University travel system. This needs to be done before you do any booking or make travel arrangements (see Before you travel ). Once approved, make all your travel arrangements: flights, hotels etc (make sure to check per diem rates before booking so you know what you can be reimbursed for) Follow the reimbursement procedures outlined (see After you travel ). Please do this as soon as possible but no later than 2 weeks after you return. You can submit up to two reimbursement requests per trip so if you would like to be reimbursed for some large expenses like airfare before you go this is possible. Add the information about your presentation to the group Zotero (note that we will not approve your expenses unless this is done) Some general guidelines to keep in mind: Travel is a privilege. All travel is supported by grant money and it is the expectation that you manage expenses in a reasonable way. The maximum reimbursement you can receive is determined by the per diem rates you can find here. For food expenses you should only request reimbursement for the amount you actually spent and you should note when meals were provided for you (i.e. lunch provided at a meeting) and not request reimbursement for this. It is okay to have personal travel at the start or end of your trip but you will need to document things and you can\u2019t be reimbursed for any personal travel. Please refer to the guidelines here. We cannot provide cash advances for travel. To manage out of pocket expenses you can use the PCARD to make bookings (see guidelines here) and submit for reimbursement for some expenses before you go. The First time you travel Setting up your account in the University travel system Visit https://travel.financialservices.arizona.edu/ Once logged in, click on the profile button (blue circle in the upper right corner) Click on Profile Settings On this page, click on Personal Information , follow the prompts and fill in all the required info, in red. The information added here must match exactly with the information in UAccess and ID Remember to click Save . Make Cynthia Barnett (barnettc@arizona.edu) a delegate: Back on the Profile Options page on the left menu click Request Delegates On this page click Add . Click everything except for Temporary , click Save . Make Garry Forger ( gforger@arizona.edu ) a travel assistant: on travel settings , click Assistants/Arrangers Search for Garry and give him access for booking. Set as the primary assistant Before you travel Submit a authorization and travel request Travel authorizations must be completed before you travel in order to get reimbursed. If you are traveling internationally there are additional steps so please start this process early. Please see the steps below and reach out to Garry if you have any questions. Step 1: Create a Travel Authorization A Travel Authorization is required when a University of Arizona Employee, Student, or Designated Campus Colleague will be on travel status (at least 35 miles outside of their duty post) and travels on behalf of University business, regardless of expenses incurred by the University. The form must be filled in the online system. https://travel.financialservices.arizona.edu/ . Remember you must be registered in the system. Step 2: Create a Travel Request To create a request from the homepage click Create (top left) and choose Start a Request and fill in all the required fields. Add the Business Travel dates with the maximum extension you think the conference would last. Example, if your conference runs from May 3 to the 6th, you would have May 2 and May 7 as travel days. If you are staying for non-work related business, indicate those dates before or after the conference/meeting. . Each time the conference has lodging options you should add this to the form. You can use another later explaining the reasons. Explain the reasons for the conference, academic development, paper presentation, University representation, etc. The organization must be (0469) Hydrology and Atmospheric Sci. For Account, indicate the account you are paid from. If you do not know this account number, check with Garry. If you are traveling internationally, you must get an International Travel Registry number to include with the request. At this site Forms click on Travel Forms , then on Travel Authorization , This has to be done at least 30 days before the travel takes place. When you click Create a Request , if you missed a required field you will get an error message and can go back and add the info. Of course get at Garry with any questions gforger@arizona.edu You will receive a notification when your request is approved. After you travel Reimbursement for Travel Please keep the following in mind for reimbursements to make the process as smooth as possible. You always need an approved travel authorization before you submit a travel reimbursement. Make sure you save receipts for everything in order to get reimbursed (see note below on food. Electronic or email receipts are best, if you have paper receipts scan them or take a picture of them with your phone. You can be reimbursed for any expenses associated with your travel to a conference or work related meeting (airplane tickets, ground transportation, airport parking, hotels, and food). For transportation like Uber or Lyfts, we need a detailed receipt that shows how much everything was charged and it includes the map. Remember ANY tips are approved up to 20% only. You can't be reimbursed for alcohol or expenses associated with any entertainment or outings that are not related to the conference/meeting. Students are expected to share a room unless it is not feasible or would put you in a position you would not feel comfortable (you should get permission from Laura in advance if this is the case). This also means that you should not request the full per diem allotment for food every day. Rather please add up all your food expenses at the end and request the correct number of lunches/dinners/breakfasts to make sure that you are compensated for what you spend but not making a profit. (The alternative is you submit receipts for every meal. I find this to be very challenging though especially when eating with other people and sharing checks so I think the best option is just to keep track of your spending and then request per diem to match) General Per Diem Information for UA . Steps for getting reimbursed: Note: You can submit up to two reimbursement requests for a single trip. If you want to be paid back sooner for big expenses you can submit one reimbursement for expenses before you go (e.g. airfare and registration) and then submit another reimbursement after you get back. Add the information about your presentation to the group zotero and email Garry a summary of where you went and why. Submit a reimbursement form to Cindy Barnett barnettc@arizona.edu with all your receipts. The form must be downloaded always from https://financialservices.arizona.edu/form/travel-expense-repor t The travel authorization on the top right must match the approved travel authorization. The form should sum automatically but it almost never works so be prepared to fill and sign it manually. It should calculate if you are using chrome, not Edge. Be aware that reimbursements generally take 1-2 months. Helpful Links: Expense report form Additional detail on University Financial Policies Guidelines for combining personal and business travel You can combine personal travel with traveling to a conference or work meeting but should keep the following in mind. Be sure to report your planned personal travel when creating your travel request. For example if your conference is May 2 to May 6, you can indicate that May 7, 8 and 9 will be personal days. If you are incorporating personal travel into travel for a conference or meeting, then the Pcard cannot be used to pay for your flights. You will need to provide cost comparisons showing the price of your tickets with and without the personal travel. You should do this at the time of booking. Please reach out to Garry or Cindy to make sure you have the needed documentation at the time of booking to make the reimbursement process smoother. Make sure to report any personal travel days as vacation on your timesheet if you are someone who fills out a timesheet. FAQ Please refer to the Travel FAQ page for the university here . Should I use Concur to book my travel? You can if you want but not if it is more expensive. We have generally found it to be cheaper not to use concur. Do I need to fill out a TA if I\u2019m driving to Phoenix? Yes TA\u2019s are needed anytime you travel more than 35 miles from the University for business reasons. Is it okay if I drive instead of fly? It depends on the location. If traveling using your own vehicle to a conference or meeting, you will be reimbursed for mileage at the rate of 65.5 cents per mile and you must show that driving your own vehicle is less expensive than other forms of transportation. Can I get a cash advance for my travel? No unfortunately you cannot. You can manage expenses by using the PCARD to book instead of your personal credit card and you can also request for one reimbursement before you go on your trip and one after. Can I book a hotel that costs more than per diem? Yes you can but you will have to pay the difference between your booking rate and what is allowed through Per Diem unless you can show that the hotel was the designated lodging for a conference. Can I upgrade my plane ticket? You will only be reimbursed for an economy flight. However, if you want to upgrade your flight at your own expense you can do that. First make your flight reservations for an economy class seat. Once you have the receipt for this saved. Then go back and upgrade your seat at your own expense. Make sure that this upgrade price is not shown on the receipt you submit for reimbursement. Do I need to do anything different for International tavel? Yes! This will require additional approval so please make sure to fill out all of the international travel forms as soon as you know that you have international travel planned.","title":"Traveling on University Business"},{"location":"admin/travel/#traveling-on-university-business","text":"Traveling to conferences and other project related meetings is generally reimbursable but it is critical that you follow the following procedures. Please note that there are steps that need to happen before you travel so plan accordingly and make sure you follow all the steps in order to be reimbursed. If it\u2019s your first time traveling with UA you will need to first activate your account in the university travel system (see The first time you travel ). For all trips you need to do the following: Get approval from Laura for the travel you are planning on doing. Create a travel authorization and travel request using the University travel system. This needs to be done before you do any booking or make travel arrangements (see Before you travel ). Once approved, make all your travel arrangements: flights, hotels etc (make sure to check per diem rates before booking so you know what you can be reimbursed for) Follow the reimbursement procedures outlined (see After you travel ). Please do this as soon as possible but no later than 2 weeks after you return. You can submit up to two reimbursement requests per trip so if you would like to be reimbursed for some large expenses like airfare before you go this is possible. Add the information about your presentation to the group Zotero (note that we will not approve your expenses unless this is done) Some general guidelines to keep in mind: Travel is a privilege. All travel is supported by grant money and it is the expectation that you manage expenses in a reasonable way. The maximum reimbursement you can receive is determined by the per diem rates you can find here. For food expenses you should only request reimbursement for the amount you actually spent and you should note when meals were provided for you (i.e. lunch provided at a meeting) and not request reimbursement for this. It is okay to have personal travel at the start or end of your trip but you will need to document things and you can\u2019t be reimbursed for any personal travel. Please refer to the guidelines here. We cannot provide cash advances for travel. To manage out of pocket expenses you can use the PCARD to make bookings (see guidelines here) and submit for reimbursement for some expenses before you go.","title":"Traveling on University Business"},{"location":"admin/travel/#the-first-time-you-travel","text":"Setting up your account in the University travel system Visit https://travel.financialservices.arizona.edu/ Once logged in, click on the profile button (blue circle in the upper right corner) Click on Profile Settings On this page, click on Personal Information , follow the prompts and fill in all the required info, in red. The information added here must match exactly with the information in UAccess and ID Remember to click Save . Make Cynthia Barnett (barnettc@arizona.edu) a delegate: Back on the Profile Options page on the left menu click Request Delegates On this page click Add . Click everything except for Temporary , click Save . Make Garry Forger ( gforger@arizona.edu ) a travel assistant: on travel settings , click Assistants/Arrangers Search for Garry and give him access for booking. Set as the primary assistant","title":"The First time you travel"},{"location":"admin/travel/#before-you-travel","text":"Submit a authorization and travel request Travel authorizations must be completed before you travel in order to get reimbursed. If you are traveling internationally there are additional steps so please start this process early. Please see the steps below and reach out to Garry if you have any questions. Step 1: Create a Travel Authorization A Travel Authorization is required when a University of Arizona Employee, Student, or Designated Campus Colleague will be on travel status (at least 35 miles outside of their duty post) and travels on behalf of University business, regardless of expenses incurred by the University. The form must be filled in the online system. https://travel.financialservices.arizona.edu/ . Remember you must be registered in the system. Step 2: Create a Travel Request To create a request from the homepage click Create (top left) and choose Start a Request and fill in all the required fields. Add the Business Travel dates with the maximum extension you think the conference would last. Example, if your conference runs from May 3 to the 6th, you would have May 2 and May 7 as travel days. If you are staying for non-work related business, indicate those dates before or after the conference/meeting. . Each time the conference has lodging options you should add this to the form. You can use another later explaining the reasons. Explain the reasons for the conference, academic development, paper presentation, University representation, etc. The organization must be (0469) Hydrology and Atmospheric Sci. For Account, indicate the account you are paid from. If you do not know this account number, check with Garry. If you are traveling internationally, you must get an International Travel Registry number to include with the request. At this site Forms click on Travel Forms , then on Travel Authorization , This has to be done at least 30 days before the travel takes place. When you click Create a Request , if you missed a required field you will get an error message and can go back and add the info. Of course get at Garry with any questions gforger@arizona.edu You will receive a notification when your request is approved.","title":"Before you travel"},{"location":"admin/travel/#after-you-travel","text":"Reimbursement for Travel Please keep the following in mind for reimbursements to make the process as smooth as possible. You always need an approved travel authorization before you submit a travel reimbursement. Make sure you save receipts for everything in order to get reimbursed (see note below on food. Electronic or email receipts are best, if you have paper receipts scan them or take a picture of them with your phone. You can be reimbursed for any expenses associated with your travel to a conference or work related meeting (airplane tickets, ground transportation, airport parking, hotels, and food). For transportation like Uber or Lyfts, we need a detailed receipt that shows how much everything was charged and it includes the map. Remember ANY tips are approved up to 20% only. You can't be reimbursed for alcohol or expenses associated with any entertainment or outings that are not related to the conference/meeting. Students are expected to share a room unless it is not feasible or would put you in a position you would not feel comfortable (you should get permission from Laura in advance if this is the case). This also means that you should not request the full per diem allotment for food every day. Rather please add up all your food expenses at the end and request the correct number of lunches/dinners/breakfasts to make sure that you are compensated for what you spend but not making a profit. (The alternative is you submit receipts for every meal. I find this to be very challenging though especially when eating with other people and sharing checks so I think the best option is just to keep track of your spending and then request per diem to match) General Per Diem Information for UA . Steps for getting reimbursed: Note: You can submit up to two reimbursement requests for a single trip. If you want to be paid back sooner for big expenses you can submit one reimbursement for expenses before you go (e.g. airfare and registration) and then submit another reimbursement after you get back. Add the information about your presentation to the group zotero and email Garry a summary of where you went and why. Submit a reimbursement form to Cindy Barnett barnettc@arizona.edu with all your receipts. The form must be downloaded always from https://financialservices.arizona.edu/form/travel-expense-repor t The travel authorization on the top right must match the approved travel authorization. The form should sum automatically but it almost never works so be prepared to fill and sign it manually. It should calculate if you are using chrome, not Edge. Be aware that reimbursements generally take 1-2 months. Helpful Links: Expense report form Additional detail on University Financial Policies","title":"After you travel"},{"location":"admin/travel/#guidelines-for-combining-personal-and-business-travel","text":"You can combine personal travel with traveling to a conference or work meeting but should keep the following in mind. Be sure to report your planned personal travel when creating your travel request. For example if your conference is May 2 to May 6, you can indicate that May 7, 8 and 9 will be personal days. If you are incorporating personal travel into travel for a conference or meeting, then the Pcard cannot be used to pay for your flights. You will need to provide cost comparisons showing the price of your tickets with and without the personal travel. You should do this at the time of booking. Please reach out to Garry or Cindy to make sure you have the needed documentation at the time of booking to make the reimbursement process smoother. Make sure to report any personal travel days as vacation on your timesheet if you are someone who fills out a timesheet.","title":"Guidelines for combining personal and business travel"},{"location":"admin/travel/#faq","text":"Please refer to the Travel FAQ page for the university here . Should I use Concur to book my travel? You can if you want but not if it is more expensive. We have generally found it to be cheaper not to use concur. Do I need to fill out a TA if I\u2019m driving to Phoenix? Yes TA\u2019s are needed anytime you travel more than 35 miles from the University for business reasons. Is it okay if I drive instead of fly? It depends on the location. If traveling using your own vehicle to a conference or meeting, you will be reimbursed for mileage at the rate of 65.5 cents per mile and you must show that driving your own vehicle is less expensive than other forms of transportation. Can I get a cash advance for my travel? No unfortunately you cannot. You can manage expenses by using the PCARD to book instead of your personal credit card and you can also request for one reimbursement before you go on your trip and one after. Can I book a hotel that costs more than per diem? Yes you can but you will have to pay the difference between your booking rate and what is allowed through Per Diem unless you can show that the hotel was the designated lodging for a conference. Can I upgrade my plane ticket? You will only be reimbursed for an economy flight. However, if you want to upgrade your flight at your own expense you can do that. First make your flight reservations for an economy class seat. Once you have the receipt for this saved. Then go back and upgrade your seat at your own expense. Make sure that this upgrade price is not shown on the receipt you submit for reimbursement. Do I need to do anything different for International tavel? Yes! This will require additional approval so please make sure to fill out all of the international travel forms as soon as you know that you have international travel planned.","title":"FAQ"},{"location":"group_meetings/","text":"Group Meetings Below is a summary of our past group meetings. They are organized by year and semester. After the conclusion of each semester, the lab group meeting coordinator will update this doc accordingly. Make sure to change the permissions on any slide links to viewer only. 2024 Spring Group Meeting Coordinator: Danielle Tadych Date Description Resources 1/11 Goal and Aspirations Slides 1/18 Danielle and Amanda research updates 2/1 Paper Storyboards 2/8 Leonardo research 2/15 Figure Day Slides 2/29 Danielle and Amanda paper storyboards 3/21 Picture day 4/4 Figure Day Slides 4/11 EGU practice 4/18 Rubab covered groundwater dependent ecosystems 4/25 Leonardo update and Rubab Interview Practice 5/2 Career Day Slides 5/16 Abstract review round robin Fall Group Meeting Coordinator: Ben West Date Description Resources 9/12 Graphs of things people were struggling with Slides 9/19 Amanda and Luis practiced talks 9/26 Ben and Patricia practiced talks 10/10 Code skills day Slides 10/24 Amanda Comps 11/7 Bonnie Colby came and talked about indigenous water rights Slides and readings 11/14 We shared code we had written that used classes as follow up to 10/10 meeting 11/21 Luis defense practice 12/5 AGU practice 12/19 Winter break goals + one interesting figure Slides 2023 Spring Group Meeting Coordinator: Patricia Puente Date Description Resources 2/2 Two Slide Research Updates and Discussion on the Salton Sea Slides for the Salton sea 2/9 Laura Practice Presentation 2/23 One Slide Research Updates 3/2 Planet Data Product and Resources with Patricia Puente Slides for Planet Demonstration 3/16 Planet APIs and Basemaps demo with Daniel Luna Slides for Planet API Demonstration 4/6 One Slide Research Updates 5/11 Building a Website with Patricia Puente Slides for Website Building Summer Group Meeting Coordinator: Patricia Puente Date Description Resources 5/18 One slide research updates Slides for Research Updates 6/8 Practice revising paragraphs from a ML paper \"Efficiently Modeling Long Sequences with Structured State Spaces\" Slides for Explanataion of the paper Slides for Paragraph Rewrite 6/29 Team introductions Introduction Slides 7/13 One slide research updates Slides for Updates Fall Group Meeting Coordinator: Danielle Tadych Date Description Resources 9/7 Discussed an article on declining global water storage Satellites reveal widespread decline in global lake water storage 9/21 Writing Assignment 9/28 Debugging how-to's, best practices, and resources with Ben West Link for Debugging Slides 10/12 Website Workshopping Day with Patricia 10/19 Picture Day and Paper Discussion of an article about ChatGPT and its applications to the Natural Sciences ChatGPT in Hydrology and Earth Sciences: Opportunities, Prospects, and Concerns 10/26 Research Updates 11/9 How Read the Docs works and division of tasks with Will and Laura 11/16 Author of Brave the Wild River, Melissa Vesigny, Q&A","title":"Group Meetings"},{"location":"group_meetings/#group-meetings","text":"Below is a summary of our past group meetings. They are organized by year and semester. After the conclusion of each semester, the lab group meeting coordinator will update this doc accordingly. Make sure to change the permissions on any slide links to viewer only.","title":"Group Meetings"},{"location":"group_meetings/#2024","text":"","title":"2024"},{"location":"group_meetings/#spring","text":"Group Meeting Coordinator: Danielle Tadych Date Description Resources 1/11 Goal and Aspirations Slides 1/18 Danielle and Amanda research updates 2/1 Paper Storyboards 2/8 Leonardo research 2/15 Figure Day Slides 2/29 Danielle and Amanda paper storyboards 3/21 Picture day 4/4 Figure Day Slides 4/11 EGU practice 4/18 Rubab covered groundwater dependent ecosystems 4/25 Leonardo update and Rubab Interview Practice 5/2 Career Day Slides 5/16 Abstract review round robin","title":"Spring"},{"location":"group_meetings/#fall","text":"Group Meeting Coordinator: Ben West Date Description Resources 9/12 Graphs of things people were struggling with Slides 9/19 Amanda and Luis practiced talks 9/26 Ben and Patricia practiced talks 10/10 Code skills day Slides 10/24 Amanda Comps 11/7 Bonnie Colby came and talked about indigenous water rights Slides and readings 11/14 We shared code we had written that used classes as follow up to 10/10 meeting 11/21 Luis defense practice 12/5 AGU practice 12/19 Winter break goals + one interesting figure Slides","title":"Fall"},{"location":"group_meetings/#2023","text":"","title":"2023"},{"location":"group_meetings/#spring_1","text":"Group Meeting Coordinator: Patricia Puente Date Description Resources 2/2 Two Slide Research Updates and Discussion on the Salton Sea Slides for the Salton sea 2/9 Laura Practice Presentation 2/23 One Slide Research Updates 3/2 Planet Data Product and Resources with Patricia Puente Slides for Planet Demonstration 3/16 Planet APIs and Basemaps demo with Daniel Luna Slides for Planet API Demonstration 4/6 One Slide Research Updates 5/11 Building a Website with Patricia Puente Slides for Website Building","title":"Spring"},{"location":"group_meetings/#summer","text":"Group Meeting Coordinator: Patricia Puente Date Description Resources 5/18 One slide research updates Slides for Research Updates 6/8 Practice revising paragraphs from a ML paper \"Efficiently Modeling Long Sequences with Structured State Spaces\" Slides for Explanataion of the paper Slides for Paragraph Rewrite 6/29 Team introductions Introduction Slides 7/13 One slide research updates Slides for Updates","title":"Summer"},{"location":"group_meetings/#fall_1","text":"Group Meeting Coordinator: Danielle Tadych Date Description Resources 9/7 Discussed an article on declining global water storage Satellites reveal widespread decline in global lake water storage 9/21 Writing Assignment 9/28 Debugging how-to's, best practices, and resources with Ben West Link for Debugging Slides 10/12 Website Workshopping Day with Patricia 10/19 Picture Day and Paper Discussion of an article about ChatGPT and its applications to the Natural Sciences ChatGPT in Hydrology and Earth Sciences: Opportunities, Prospects, and Concerns 10/26 Research Updates 11/9 How Read the Docs works and division of tasks with Will and Laura 11/16 Author of Brave the Wild River, Melissa Vesigny, Q&A","title":"Fall"},{"location":"how_to_notes/git_workflows/","text":"Git Workflows What is git Git is version control software used to enable collaboration between many people on a single project. This youtube video gives a good synopsis of what git is. Getting started with git Making an account Git is a software technology that is independent of remote repository systems like github, gitlab and bitbucket. However it is almsot used in conjunction with one of these services. This project, and most others, are managed through github. If you don't already, you'll need a github account. You can make a github account here! SSH keys In most cases, to start using git on a new computer you need to set up a new SSH key. An SSH (secure shell) key is a method of security control where an ecnrypted key is generated and stored on a client and service. These keys are used when the client trys to connect to the service to verify that both parties are who they say they are. You'll need an SSH key to clone using SSH rather than https, SSH is genreally considered to be a safer method and is therefore reccomended. Follow these directions to generate a new SSH key. You can choose to add a passphrase to your SSH key or not. If you do choose to add a passphrase it is recommended that you add the key to an SSH-agent. This will make it so you don't have to type in your passphrase everytime you interact with git. Follow these instructions to add your key to an SSH agent Now that you've generated an SSH key, follow these instructions to add your SSH key to your github account Helpful git commands Here is a decent git cheat sheet. This reference covers the basic usage of git. Be aware that the git rabbit hole is deep and things can get complciated fast, so don't be affraid to google specific issues you experience. Basic pull request workflow Below is outlined a typical workflow for opening pull requests (PR) and participating in code review. This applies when you are working in a shared repo (like this one \ud83d\ude09). These instructions assume you already have cloned the repo ( git clone <repo address> ) and have made and committed changes ( git add <changed file> and git commit -m <commit message> ) Before you push your code to open a PR it is a good practice to pull from the main branch to make sure you have the most up-to-date version of the code: On your working branch git pull origin main Resolve any merge conflicts Push your code to the remote git push or if it's your first push on this branch git push --set-upstream origin <branch name> Open a new pull request. If you navigate to the repo's page in github shortly after pushing you should see a large green button prompting you to open a PR. Otherwise you need to click the PR tab, select New Pull Request and configure main <- <your branch> The above steps will open a new PR form, please write a description of your changes. If you are working on an established project it's helfpul to include the steps you used to test your code as well as screen shots. Assign one or more reviewers, or otherwise let folks know you have a PR ready for review Once your code has been reviewed there is likely some changes you need to make, you can make changes on your current branch and push them to the remote, the PR will pick up the changes. It's good to make make sure you have the most recent code from main before each push back to the PR ( git pull origin main and then resolve merge conflicts if they exist) Once your PR is approved, either you or someone from the project will merge your changes in. This varies from project to project so just ask.","title":"Git Workflows"},{"location":"how_to_notes/git_workflows/#git-workflows","text":"","title":"Git Workflows"},{"location":"how_to_notes/git_workflows/#what-is-git","text":"Git is version control software used to enable collaboration between many people on a single project. This youtube video gives a good synopsis of what git is.","title":"What is git"},{"location":"how_to_notes/git_workflows/#getting-started-with-git","text":"","title":"Getting started with git"},{"location":"how_to_notes/git_workflows/#making-an-account","text":"Git is a software technology that is independent of remote repository systems like github, gitlab and bitbucket. However it is almsot used in conjunction with one of these services. This project, and most others, are managed through github. If you don't already, you'll need a github account. You can make a github account here!","title":"Making an account"},{"location":"how_to_notes/git_workflows/#ssh-keys","text":"In most cases, to start using git on a new computer you need to set up a new SSH key. An SSH (secure shell) key is a method of security control where an ecnrypted key is generated and stored on a client and service. These keys are used when the client trys to connect to the service to verify that both parties are who they say they are. You'll need an SSH key to clone using SSH rather than https, SSH is genreally considered to be a safer method and is therefore reccomended. Follow these directions to generate a new SSH key. You can choose to add a passphrase to your SSH key or not. If you do choose to add a passphrase it is recommended that you add the key to an SSH-agent. This will make it so you don't have to type in your passphrase everytime you interact with git. Follow these instructions to add your key to an SSH agent Now that you've generated an SSH key, follow these instructions to add your SSH key to your github account","title":"SSH keys"},{"location":"how_to_notes/git_workflows/#helpful-git-commands","text":"Here is a decent git cheat sheet. This reference covers the basic usage of git. Be aware that the git rabbit hole is deep and things can get complciated fast, so don't be affraid to google specific issues you experience.","title":"Helpful git commands"},{"location":"how_to_notes/git_workflows/#basic-pull-request-workflow","text":"Below is outlined a typical workflow for opening pull requests (PR) and participating in code review. This applies when you are working in a shared repo (like this one \ud83d\ude09). These instructions assume you already have cloned the repo ( git clone <repo address> ) and have made and committed changes ( git add <changed file> and git commit -m <commit message> ) Before you push your code to open a PR it is a good practice to pull from the main branch to make sure you have the most up-to-date version of the code: On your working branch git pull origin main Resolve any merge conflicts Push your code to the remote git push or if it's your first push on this branch git push --set-upstream origin <branch name> Open a new pull request. If you navigate to the repo's page in github shortly after pushing you should see a large green button prompting you to open a PR. Otherwise you need to click the PR tab, select New Pull Request and configure main <- <your branch> The above steps will open a new PR form, please write a description of your changes. If you are working on an established project it's helfpul to include the steps you used to test your code as well as screen shots. Assign one or more reviewers, or otherwise let folks know you have a PR ready for review Once your code has been reviewed there is likely some changes you need to make, you can make changes on your current branch and push them to the remote, the PR will pick up the changes. It's good to make make sure you have the most recent code from main before each push back to the PR ( git pull origin main and then resolve merge conflicts if they exist) Once your PR is approved, either you or someone from the project will merge your changes in. This varies from project to project so just ask.","title":"Basic pull request workflow"},{"location":"how_to_notes/HPC_systems/UA_HPC/","text":"UA HPC Information Laura Condon, Oct, 2018 (Updated) Quinn Hull, Nov 2020 (Updated) Rachel Spinti, Feb 2021 Description Running notes with links and tips for running on the UA HPC system. Feel free to update and add to this document as needed. UA has multiple HPC systems. These docs contain instructions for both Ocelote Quick Start A good starting point. Tutorial highlights essential steps in running a basic job on HPC Ocelote. How to log in What a login node is What a job scheduler is How to access software How to run a job on HPC https://public.confluence.arizona.edu/display/UAHPC/Ocelote+Quick+Start Puma Quick Start: Tutorial below highlights essential steps in running a basic job on HPC Puma. How to log in What a login node is How to access software (Note: it is different than Ocelote, see https://public.confluence.arizona.edu/display/UAHPC/Puma+Quick+Start#PumaQuickStart-AccessingSoftware ) How to write a SLURM script How to run a job on Puma https://public.confluence.arizona.edu/display/UAHPC/Puma+Quick+Start UITS Account Management To set-up a UA HPC account if you have not activated one. Sign-in and go to \u201cManage Your Accounts\u201d https://account.arizona.edu/welcome HPC portal https://portal.hpc.arizona.edu Logging in _ssh username@hpc.arizona.edu (e.g. roberthull@hpc.arizona.edu)_ Should give you a choice to connect to Ocelote - if not you can type menuon to get the menu Type ocelote to enter the real HPC system HPC Browser Dashboard (OnDemand) https://ood.hpc.arizona.edu/pun/sys/dashboard A GUI that allows you to monitor jobs and access files outside the terminal. Can be easier to view and/or edit files this way than in command line. How to run Python in a Jupiter notebook on UAHPC It might be easiest to run Python using a Jupyter Notebook, for which UAHPC has developed super convenient GUIs. Check here for a great tutorial. Link for computing account management https://account.arizona.edu/welcome Instructions for setting up and account https://public.confluence.arizona.edu/display/UAHPC/Account+Creation Accessing Software via Module commands https://docs.hpc.arizona.edu/display/UAHPC/Accessing+Software Module list (gives loaded modules) Module avail (gives list of all modules) Module show NAME (gives info on a specific module Storage and limits https://docs.hpc.arizona.edu/display/UAHPC/Allocation+and+Limits 50GB on home/uxx/netid (ex home/u8/roberthull) 200GB (no Backups) on extra/netid /temp - 840GB available per node. You can use this during the job and then do a final write to a shared array. (You cannot permanently save anything to this location) No more than 600files/GB ~1.6 MB per file There is a total time limitation for our group (36,000 hours/month), use command va to check the remaining time of the month. Uploading files to UAHPC: If Globus can be used on UAHPC you should. I (Ben West) am not sure if it can because I have not tried. Directions are in general info. https://public.confluence.arizona.edu/display/UAHPC/Transferring+Files (small files) Use the GUI web terminal https://ood.hpc.arizona.edu/pun/sys/dashboard Log on Click \u2018Files\u2019 in top left corner Select Directory to which you want to add files (e.g. Click Upload and navigate to the local file location. To use the file, navigate to the file location by clicking Open in terminal (big files <100 GB). Use sftp, scp or rsync using filexfer.hpc.arizona.edu if you can't use Globus Copy your files through through the transfer node like below: _scp -rp file.txt &lt;netid>[@filexfer.hpc.arizona.edu](mailto:lecondon@filexfer.hpc.arizona.edu):&lt;directory>_ Example: If I (roberthull) want to upload a python script (test.py) to my home directory (/home/u8/roberthull) the command would look like the below _scp -rp test.py roberthull[@filexfer.hpc.arizona.edu](mailto:lecondon@filexfer.hpc.arizona.edu):_/home/u8/roberthull *** Remember that on all HPC systems your files will be purged after a certain amount of time it is your job to be aware of the purge policies and copy your outputs somewhere appropriate for longer term storage. *** Note as shown above scp requires http://filexfer.hpc.arizona.edu/ rather than hpc.arizona.edu ParFlow on UAHPC _If you want to run with one of the versions of ParFlow that has already been build just add these lines to your bashrc and make sure that you aren\u2019t also setting \u2018PARFLOW_DIR\u2019 to the appropriate location in your bashrc file. _ ssh [username@hpc.arizona.edu](mailto:username@hpc.arizona.edu) cd $HOME vi ~/.bashrc # Add the following lines to this file module load unsupported module load lecondon/parflow/latest # You can check if things look right using the commands which parflow echo $PARFLOW_DIR You can also install parflow using the instructions in general info Time Allocation Each PI has a finite amount of time allocation each month. After each run completes, the _run_name.o####### _file will show the amount of time left in the standard queue each month. Each time you submit a script, the system makes sure that your request can be fulfilled in the given queue - otherwise, it returns an error message. If you\u2019re requesting more time than you have for the rest of the month, you can: 1) reduce the amount of time you are requesting, 2) submit it to the windfall queue, or 3) wait until the end of the month and try again next month. Your advisor may not be okay with you trying option 3. Accessing and using software (like Python) General: https://public.confluence.arizona.edu/display/UAHPC/Accessing+Software Python: https://public.confluence.arizona.edu/display/UAHPC/Using+and+Installing+Python Conda: https://public.confluence.arizona.edu/display/UAHPC/Open+On+Demand Summary: Many software are installed on the server (> 100). Note that both Python and Anaconda exist, which allow for the installation of Python packages and usage of conda virtual environments. Python: There are four versions of Python on Ocelote, (up to Python 3.6.5). Python 3.6.5 is loaded with \u2018module load python\u2019, and includes machine learning packages like Tensorflow. (Does it include PyTorch?) There are three versions of Python in ElGato (up to Python 3.8.0). Python 3.8.0 is loaded with \u2018module load python/3.8/3.8.0\u2019 There are two versions of Python in Puma (up to Python 3.8.2) Note that for DL software, Puma is best https://public.confluence.arizona.edu/display/UAHPC/Puma+Quick+Start How to do a basic script in Python Log in via terminal or bash see the \"Logging In\" section Find Python modules module avail python This will list availalbe python modules. e.g. python/3.6/3.65 Load Python module load python/3.6/3.65 Add/Create/Locate the script you want to run pytorch_helloworld.py -> print(\"hello world\") Create and edit pbs script (note important fields) touch pytorch_helloworld.pbs vim pytorch_helloworld.pbs Example PBS workflow # -------------------------------------------------------------- ### PART 1: Requests resources to run your job. # -------------------------------------------------------------- ### Optional. Set the job name #PBS -N pytorch_helloworld ### REQUIRED. Specify the PI group for this job #PBS -W group_list=lecondon ### Optional. Request email when job begins and ends ### PBS -m bea ### Optional. Specify email address to use for notification ### PBS -M roberthull@email.arizona.edu ### REQUIRED. Set the queue for your job. #PBS -q windfall ### REQUIRED. Set the number of nodes, cores and memory that will be used for this job ### pcmem is optional as it defaults to 6gb per core. Only required for high memory = 42gb. #PBS -l select=1:ncpus=1:mem=6gb:pcmem=6gb ### REQUIRED. Specify \"wallclock time\" required for this job, hhh:mm:ss #PBS -l walltime=0:1:0 ### Optional. cput = time x ncpus. Default: cput = walltime x ncpus. #PBS -l cput=0:1:0 # -------------------------------------------------------------- ### PART 2: Executes bash commands to run your job # -------------------------------------------------------------- ### Load required modules/libraries if needed module load python/3.6/3.6.5 ### change to your scripts directory cd ~/python ### run your work python3 pytorch_helloworld.py sleep 10i Submit job via qsub qsub pytorch_helloworld.pbs3592178.head1.cm.cluster Follow job progression via qstat View results of job <jobname>.o<number> for results, ` .e for errors Look at results: vim pytorch_helloworld.o3692196 -> hello world If there are errors use: vim pytorch_helloworld.e3692196 How to run a more complicated Python script in terminal on UAHPC Log in via terminal or bash (go to Ocelote) See the \"Logging in\" section Log in via terminal or bash see the \"Logging In\" section Find Python modules module avail python This will list availalbe python modules. e.g. python/3.6/3.65 Load Python module load python/3.6/3.65 Take care that you load the version of python that you need. That includes considerations of what packages are available via UAHPC, and how you\u2019ve installed custom packages in your virtual environment Also, you can check what python versions are available and/or are loaded using module list Activate your virtual environment (create it if necessary) virtualenv --system-site-packages ~/mypyenv_oc_py.3.6 source ~/mypyenv_oc_py3.6/bin/activate Take care to use a virtual environment compatible with your version of python and with your super computer of choice (Ocelote v ElGato) Add/Create/Locate the script you want to run Pytorch_gpus.py \u2192 runs a simple gpu script Note if you use custom packages you will need to manually reference the location of your virtual environment in the following way within your python script: Check which packages you have in your virtual environment, pip list (note that python/3.6/3.6.5 for ocelote has almost everything you would ever want to have\u2026) Install any packages that you think you need to run your desired script pip install <name_of_package> Create and edit pbs script (note important fields) #!/bin/bash # -------------------------------------------------------------- ### PART 1: Requests resources to run your job. # -------------------------------------------------------------- ### Optinal. Set the job name #PBS -N pytorch_gpus_n1 ### REQUIRED. Specify the PI group for this job #PBS -W group_list=lecondon ### Optional. Request email when job begins and ends ### PBS -m bea ### Optional. Specify email address to use for notification ### PBS -M &lt;YOUR NETID>@email.arizona.edu ### REQUIRED. Set the queue for your job. #PBS -q windfall ### REQUIRED. Set the number of nodes, cores and memory that will be used for this job ### pcmem is optional as it defaults to 6gb per core. Only required for high memory = 42gb. #PBS -l select=1:ncpus=1:mem=6gb ### REQUIRED. Specify \"wallclock time\" required for this job, hhh:mm:ss #PBS -l walltime=0:1:0 ### Optional. cput = time x ncpus. Default: cput = walltime x ncpus. #PBS -l cput=0:1:0 # -------------------------------------------------------------- ### PART 2: Executes bash commands to run your job # -------------------------------------------------------------- ### Load required modules/libraries if needed module load python/3.6/3.6.5 #### Activate virtual environment source ~/mypyenv_oc_py3.6/bin/activate ### change to your script\u2019s directory cd ~/python ### Run your work python3 pytorch_gpus.py sleep 10 A quick note about Requesting Resources and Node Types (Standard, GPU, or High Memory) via the select statement. The basic `select statement is: #PBS -l select=x:ncpus=Y:mem=Zgb Where: x = The number of nodes or units of the resources required Y = The number of cores (individual processors) required on each node Z = The amount of memory (in mb or gb) required on each node \u2018Normal\u2019 Requests Ocelote For Ocelote all of the standard nodes have 28 cores and 6GB per core, pcmem=6gb can be added to the line or left off, and it will default to 6gm. The following select statement would request one complete node: #PBS -l select=28:ncpus=28:pcmem=6gb This is an example of a job that uses two nodes #PBS -l select=2:ncpus=28:mem=16gb El Gato On El Gato all of the standard nodes of 16 cores and 4GB per core. The maximum available memory on a standard El Gato node is 62GB, leaving the differences for the operating system. The following select statement would request one complete node #PBS -l select=1:ncpus=16:pcmem=4gb \u2018GPU\u2019 Requests Ocelote On Ocelote there are 46 Nvidia P100 nodes available with 28 cores and 224 GB of memory. Users of these nodes can use either their standard allocation of cpu hours or windfall time. Jobs that do not need GPU's will not run on them, including windfall jobs taht do no need the GPU. EAch job will have exclusive access to the node to prevent contention between the CPU cores and the GPU. Each group is limited to ten GPU nodes concurrently. This requests a CentOs 7 GPU node: #PBS -l select=1:ncpus=28:mem=224gb:np100s=1:os7=True or #PBS -l select=1:ncpus=28:mem=224gb:ngpus=1:os7=True This requests a CentOS 6 GPU node: #PBS -l select=1:ncpus=28:mem=224gb:np100s=1 Ocelote has a single node with two GPUs that may be requested with #PBS -l select=1:ncpus=28:mem=224gb:np100s=2 El Gato Unlike Ocelote, El Gato has cgroups enabled which allows for selecting a partial GPU node. Memory requests should be scaled by ncpusx16gb. To request a full GPU node: #PBS -l select=1:ncpus=16:mem=250gb:ngpus=1:pcmem=16gb To request a single node with two GPUs: #PBS -l select=1:ncpus=16:mem=250gb:ngpus=2:pcmem=16gb An example of a GPU pbs using pytorch is as follows #!/bin/bash # -------------------------------------------------------------- ### PART 1: Requests resources to run your job. # -------------------------------------------------------------- #PBS -q standard #PBS -l select=1:ncpus=28:mem=168gb:pcmem=6gb:np100s=1:os7=True ### Specify a name for the job #PBS -N pytorch_gpus_single3 ### Specify the group name #PBS -W group_list=lecondon ### Walltime is how long your job will run #PBS -l walltime=00:05:00 # -------------------------------------------------------------- ### PART 2: Executes bash commands to run your job # -------------------------------------------------------------- ### Load required modules/libraries if needed module load python/3.6/3.6.5 ### In case needed for gpu..? module load pytorch/nvidia/20.01 ### Activate virtual environment source ~/mypyenv_oc_py3.6/bin/activate ### change to your script\u2019s directory cd ~/python ### Run your work singularity exec --nv /cm/shared/uaapps/pytorch/20.01/nvidia-pytorch.20.01-py3.simg python pytorch_gpus.py sleep 10 Submit job via qsub qsub pytorch_gpus_n1.pbs2708168.head1.cm.cluster Follow job progression via qstat qstat pbs2708168 View results of job <jobname>.o<number> for results, <jobname>.e<number> for errors","title":"UA HPC Information"},{"location":"how_to_notes/HPC_systems/UA_HPC/#ua-hpc-information","text":"Laura Condon, Oct, 2018 (Updated) Quinn Hull, Nov 2020 (Updated) Rachel Spinti, Feb 2021","title":"UA HPC Information"},{"location":"how_to_notes/HPC_systems/UA_HPC/#description","text":"Running notes with links and tips for running on the UA HPC system. Feel free to update and add to this document as needed. UA has multiple HPC systems. These docs contain instructions for both","title":"Description"},{"location":"how_to_notes/HPC_systems/UA_HPC/#ocelote-quick-start","text":"A good starting point. Tutorial highlights essential steps in running a basic job on HPC Ocelote. How to log in What a login node is What a job scheduler is How to access software How to run a job on HPC https://public.confluence.arizona.edu/display/UAHPC/Ocelote+Quick+Start","title":"Ocelote Quick Start"},{"location":"how_to_notes/HPC_systems/UA_HPC/#puma-quick-start","text":"Tutorial below highlights essential steps in running a basic job on HPC Puma. How to log in What a login node is How to access software (Note: it is different than Ocelote, see https://public.confluence.arizona.edu/display/UAHPC/Puma+Quick+Start#PumaQuickStart-AccessingSoftware ) How to write a SLURM script How to run a job on Puma https://public.confluence.arizona.edu/display/UAHPC/Puma+Quick+Start","title":"Puma Quick Start:"},{"location":"how_to_notes/HPC_systems/UA_HPC/#uits-account-management","text":"To set-up a UA HPC account if you have not activated one. Sign-in and go to \u201cManage Your Accounts\u201d https://account.arizona.edu/welcome","title":"UITS Account Management"},{"location":"how_to_notes/HPC_systems/UA_HPC/#hpc-portal","text":"https://portal.hpc.arizona.edu","title":"HPC portal"},{"location":"how_to_notes/HPC_systems/UA_HPC/#logging-in","text":"_ssh username@hpc.arizona.edu (e.g. roberthull@hpc.arizona.edu)_ Should give you a choice to connect to Ocelote - if not you can type menuon to get the menu Type ocelote to enter the real HPC system","title":"Logging in"},{"location":"how_to_notes/HPC_systems/UA_HPC/#hpc-browser-dashboard-ondemand","text":"https://ood.hpc.arizona.edu/pun/sys/dashboard A GUI that allows you to monitor jobs and access files outside the terminal. Can be easier to view and/or edit files this way than in command line.","title":"HPC Browser Dashboard (OnDemand)"},{"location":"how_to_notes/HPC_systems/UA_HPC/#how-to-run-python-in-a-jupiter-notebook-on-uahpc","text":"It might be easiest to run Python using a Jupyter Notebook, for which UAHPC has developed super convenient GUIs. Check here for a great tutorial.","title":"How to run Python in a Jupiter notebook on UAHPC"},{"location":"how_to_notes/HPC_systems/UA_HPC/#link-for-computing-account-management","text":"https://account.arizona.edu/welcome","title":"Link for computing account management"},{"location":"how_to_notes/HPC_systems/UA_HPC/#instructions-for-setting-up-and-account","text":"https://public.confluence.arizona.edu/display/UAHPC/Account+Creation","title":"Instructions for setting up and account"},{"location":"how_to_notes/HPC_systems/UA_HPC/#accessing-software-via-module-commands","text":"https://docs.hpc.arizona.edu/display/UAHPC/Accessing+Software Module list (gives loaded modules) Module avail (gives list of all modules) Module show NAME (gives info on a specific module","title":"Accessing Software via Module commands"},{"location":"how_to_notes/HPC_systems/UA_HPC/#storage-and-limits","text":"https://docs.hpc.arizona.edu/display/UAHPC/Allocation+and+Limits 50GB on home/uxx/netid (ex home/u8/roberthull) 200GB (no Backups) on extra/netid /temp - 840GB available per node. You can use this during the job and then do a final write to a shared array. (You cannot permanently save anything to this location) No more than 600files/GB ~1.6 MB per file There is a total time limitation for our group (36,000 hours/month), use command va to check the remaining time of the month.","title":"Storage and limits"},{"location":"how_to_notes/HPC_systems/UA_HPC/#uploading-files-to-uahpc","text":"If Globus can be used on UAHPC you should. I (Ben West) am not sure if it can because I have not tried. Directions are in general info. https://public.confluence.arizona.edu/display/UAHPC/Transferring+Files (small files) Use the GUI web terminal https://ood.hpc.arizona.edu/pun/sys/dashboard Log on Click \u2018Files\u2019 in top left corner Select Directory to which you want to add files (e.g. Click Upload and navigate to the local file location. To use the file, navigate to the file location by clicking Open in terminal (big files <100 GB). Use sftp, scp or rsync using filexfer.hpc.arizona.edu if you can't use Globus Copy your files through through the transfer node like below: _scp -rp file.txt &lt;netid>[@filexfer.hpc.arizona.edu](mailto:lecondon@filexfer.hpc.arizona.edu):&lt;directory>_ Example: If I (roberthull) want to upload a python script (test.py) to my home directory (/home/u8/roberthull) the command would look like the below _scp -rp test.py roberthull[@filexfer.hpc.arizona.edu](mailto:lecondon@filexfer.hpc.arizona.edu):_/home/u8/roberthull *** Remember that on all HPC systems your files will be purged after a certain amount of time it is your job to be aware of the purge policies and copy your outputs somewhere appropriate for longer term storage. *** Note as shown above scp requires http://filexfer.hpc.arizona.edu/ rather than hpc.arizona.edu","title":"Uploading files to UAHPC:"},{"location":"how_to_notes/HPC_systems/UA_HPC/#parflow-on-uahpc","text":"_If you want to run with one of the versions of ParFlow that has already been build just add these lines to your bashrc and make sure that you aren\u2019t also setting \u2018PARFLOW_DIR\u2019 to the appropriate location in your bashrc file. _ ssh [username@hpc.arizona.edu](mailto:username@hpc.arizona.edu) cd $HOME vi ~/.bashrc # Add the following lines to this file module load unsupported module load lecondon/parflow/latest # You can check if things look right using the commands which parflow echo $PARFLOW_DIR You can also install parflow using the instructions in general info","title":"ParFlow on UAHPC"},{"location":"how_to_notes/HPC_systems/UA_HPC/#time-allocation","text":"Each PI has a finite amount of time allocation each month. After each run completes, the _run_name.o####### _file will show the amount of time left in the standard queue each month. Each time you submit a script, the system makes sure that your request can be fulfilled in the given queue - otherwise, it returns an error message. If you\u2019re requesting more time than you have for the rest of the month, you can: 1) reduce the amount of time you are requesting, 2) submit it to the windfall queue, or 3) wait until the end of the month and try again next month. Your advisor may not be okay with you trying option 3.","title":"Time Allocation"},{"location":"how_to_notes/HPC_systems/UA_HPC/#accessing-and-using-software-like-python","text":"General: https://public.confluence.arizona.edu/display/UAHPC/Accessing+Software Python: https://public.confluence.arizona.edu/display/UAHPC/Using+and+Installing+Python Conda: https://public.confluence.arizona.edu/display/UAHPC/Open+On+Demand Summary: Many software are installed on the server (> 100). Note that both Python and Anaconda exist, which allow for the installation of Python packages and usage of conda virtual environments.","title":"Accessing and using software (like Python)"},{"location":"how_to_notes/HPC_systems/UA_HPC/#python","text":"There are four versions of Python on Ocelote, (up to Python 3.6.5). Python 3.6.5 is loaded with \u2018module load python\u2019, and includes machine learning packages like Tensorflow. (Does it include PyTorch?) There are three versions of Python in ElGato (up to Python 3.8.0). Python 3.8.0 is loaded with \u2018module load python/3.8/3.8.0\u2019 There are two versions of Python in Puma (up to Python 3.8.2) Note that for DL software, Puma is best https://public.confluence.arizona.edu/display/UAHPC/Puma+Quick+Start","title":"Python:"},{"location":"how_to_notes/HPC_systems/UA_HPC/#how-to-do-a-basic-script-in-python","text":"Log in via terminal or bash see the \"Logging In\" section Find Python modules module avail python This will list availalbe python modules. e.g. python/3.6/3.65 Load Python module load python/3.6/3.65 Add/Create/Locate the script you want to run pytorch_helloworld.py -> print(\"hello world\") Create and edit pbs script (note important fields) touch pytorch_helloworld.pbs vim pytorch_helloworld.pbs","title":"How to do a basic script in Python"},{"location":"how_to_notes/HPC_systems/UA_HPC/#example-pbs-workflow","text":"# -------------------------------------------------------------- ### PART 1: Requests resources to run your job. # -------------------------------------------------------------- ### Optional. Set the job name #PBS -N pytorch_helloworld ### REQUIRED. Specify the PI group for this job #PBS -W group_list=lecondon ### Optional. Request email when job begins and ends ### PBS -m bea ### Optional. Specify email address to use for notification ### PBS -M roberthull@email.arizona.edu ### REQUIRED. Set the queue for your job. #PBS -q windfall ### REQUIRED. Set the number of nodes, cores and memory that will be used for this job ### pcmem is optional as it defaults to 6gb per core. Only required for high memory = 42gb. #PBS -l select=1:ncpus=1:mem=6gb:pcmem=6gb ### REQUIRED. Specify \"wallclock time\" required for this job, hhh:mm:ss #PBS -l walltime=0:1:0 ### Optional. cput = time x ncpus. Default: cput = walltime x ncpus. #PBS -l cput=0:1:0 # -------------------------------------------------------------- ### PART 2: Executes bash commands to run your job # -------------------------------------------------------------- ### Load required modules/libraries if needed module load python/3.6/3.6.5 ### change to your scripts directory cd ~/python ### run your work python3 pytorch_helloworld.py sleep 10i Submit job via qsub qsub pytorch_helloworld.pbs3592178.head1.cm.cluster Follow job progression via qstat View results of job <jobname>.o<number> for results, ` .e for errors Look at results: vim pytorch_helloworld.o3692196 -> hello world If there are errors use: vim pytorch_helloworld.e3692196","title":"Example PBS workflow"},{"location":"how_to_notes/HPC_systems/UA_HPC/#how-to-run-a-more-complicated-python-script-in-terminal-on-uahpc","text":"Log in via terminal or bash (go to Ocelote) See the \"Logging in\" section Log in via terminal or bash see the \"Logging In\" section Find Python modules module avail python This will list availalbe python modules. e.g. python/3.6/3.65 Load Python module load python/3.6/3.65 Take care that you load the version of python that you need. That includes considerations of what packages are available via UAHPC, and how you\u2019ve installed custom packages in your virtual environment Also, you can check what python versions are available and/or are loaded using module list Activate your virtual environment (create it if necessary) virtualenv --system-site-packages ~/mypyenv_oc_py.3.6 source ~/mypyenv_oc_py3.6/bin/activate Take care to use a virtual environment compatible with your version of python and with your super computer of choice (Ocelote v ElGato) Add/Create/Locate the script you want to run Pytorch_gpus.py \u2192 runs a simple gpu script Note if you use custom packages you will need to manually reference the location of your virtual environment in the following way within your python script: Check which packages you have in your virtual environment, pip list (note that python/3.6/3.6.5 for ocelote has almost everything you would ever want to have\u2026) Install any packages that you think you need to run your desired script pip install <name_of_package> Create and edit pbs script (note important fields) #!/bin/bash # -------------------------------------------------------------- ### PART 1: Requests resources to run your job. # -------------------------------------------------------------- ### Optinal. Set the job name #PBS -N pytorch_gpus_n1 ### REQUIRED. Specify the PI group for this job #PBS -W group_list=lecondon ### Optional. Request email when job begins and ends ### PBS -m bea ### Optional. Specify email address to use for notification ### PBS -M &lt;YOUR NETID>@email.arizona.edu ### REQUIRED. Set the queue for your job. #PBS -q windfall ### REQUIRED. Set the number of nodes, cores and memory that will be used for this job ### pcmem is optional as it defaults to 6gb per core. Only required for high memory = 42gb. #PBS -l select=1:ncpus=1:mem=6gb ### REQUIRED. Specify \"wallclock time\" required for this job, hhh:mm:ss #PBS -l walltime=0:1:0 ### Optional. cput = time x ncpus. Default: cput = walltime x ncpus. #PBS -l cput=0:1:0 # -------------------------------------------------------------- ### PART 2: Executes bash commands to run your job # -------------------------------------------------------------- ### Load required modules/libraries if needed module load python/3.6/3.6.5 #### Activate virtual environment source ~/mypyenv_oc_py3.6/bin/activate ### change to your script\u2019s directory cd ~/python ### Run your work python3 pytorch_gpus.py sleep 10 A quick note about Requesting Resources and Node Types (Standard, GPU, or High Memory) via the select statement. The basic `select statement is: #PBS -l select=x:ncpus=Y:mem=Zgb Where: x = The number of nodes or units of the resources required Y = The number of cores (individual processors) required on each node Z = The amount of memory (in mb or gb) required on each node \u2018Normal\u2019 Requests Ocelote For Ocelote all of the standard nodes have 28 cores and 6GB per core, pcmem=6gb can be added to the line or left off, and it will default to 6gm. The following select statement would request one complete node: #PBS -l select=28:ncpus=28:pcmem=6gb This is an example of a job that uses two nodes #PBS -l select=2:ncpus=28:mem=16gb El Gato On El Gato all of the standard nodes of 16 cores and 4GB per core. The maximum available memory on a standard El Gato node is 62GB, leaving the differences for the operating system. The following select statement would request one complete node #PBS -l select=1:ncpus=16:pcmem=4gb \u2018GPU\u2019 Requests Ocelote On Ocelote there are 46 Nvidia P100 nodes available with 28 cores and 224 GB of memory. Users of these nodes can use either their standard allocation of cpu hours or windfall time. Jobs that do not need GPU's will not run on them, including windfall jobs taht do no need the GPU. EAch job will have exclusive access to the node to prevent contention between the CPU cores and the GPU. Each group is limited to ten GPU nodes concurrently. This requests a CentOs 7 GPU node: #PBS -l select=1:ncpus=28:mem=224gb:np100s=1:os7=True or #PBS -l select=1:ncpus=28:mem=224gb:ngpus=1:os7=True This requests a CentOS 6 GPU node: #PBS -l select=1:ncpus=28:mem=224gb:np100s=1 Ocelote has a single node with two GPUs that may be requested with #PBS -l select=1:ncpus=28:mem=224gb:np100s=2 El Gato Unlike Ocelote, El Gato has cgroups enabled which allows for selecting a partial GPU node. Memory requests should be scaled by ncpusx16gb. To request a full GPU node: #PBS -l select=1:ncpus=16:mem=250gb:ngpus=1:pcmem=16gb To request a single node with two GPUs: #PBS -l select=1:ncpus=16:mem=250gb:ngpus=2:pcmem=16gb An example of a GPU pbs using pytorch is as follows #!/bin/bash # -------------------------------------------------------------- ### PART 1: Requests resources to run your job. # -------------------------------------------------------------- #PBS -q standard #PBS -l select=1:ncpus=28:mem=168gb:pcmem=6gb:np100s=1:os7=True ### Specify a name for the job #PBS -N pytorch_gpus_single3 ### Specify the group name #PBS -W group_list=lecondon ### Walltime is how long your job will run #PBS -l walltime=00:05:00 # -------------------------------------------------------------- ### PART 2: Executes bash commands to run your job # -------------------------------------------------------------- ### Load required modules/libraries if needed module load python/3.6/3.6.5 ### In case needed for gpu..? module load pytorch/nvidia/20.01 ### Activate virtual environment source ~/mypyenv_oc_py3.6/bin/activate ### change to your script\u2019s directory cd ~/python ### Run your work singularity exec --nv /cm/shared/uaapps/pytorch/20.01/nvidia-pytorch.20.01-py3.simg python pytorch_gpus.py sleep 10 Submit job via qsub qsub pytorch_gpus_n1.pbs2708168.head1.cm.cluster Follow job progression via qstat qstat pbs2708168 View results of job <jobname>.o<number> for results, <jobname>.e<number> for errors","title":"How to run a more complicated Python script in terminal on UAHPC"},{"location":"how_to_notes/HPC_systems/cheyenne/","text":"Running on Cheyenne (NCAR) **Amanda Triplett ** Original: 12/2019 Update: 08/2020 Description ** **This gives an overview with examples of how to remotely access HPC resources at NCAR (specifically Cheyenne), update your bash profile and modules, an example job_script with the commands Cheyenne needs to run, how to run and check your job as well as useful information to transfer and edit your files. Software No specific software is needed, you do need an account with NCAR and an HPC time allotment in order to access resources. Everything can be done through your terminal. Accessing and logging in to Cheyenne Go to your scratch folder 4. Every user starts with a scratch folder under their username, once you navigate here, you can create file directories just as you do on your own computer 4. cd /glade/scratch/\u201dyour_username\u201d 5. Mkdir \u201cnew_directory_name\u201d Updating your bash profile and loading modules for Parflow Make any necessary updates to your bash profile, directions below are required if you want to run ParFlow - you should only need to do this once and make sure you go to your scratch folder. The first time you do it you will have to either source your bash profile or log out then back in. first vim ~/.bash_profile export PARFLOW_DIR=/glade/p/univ/ucsm0002/parflow3.7_0826_2020/parflow export HYPRE_DIR=/glade/p/univ/ucsm0002/hypre/2.10.1 export SILO_DIR=/glade/p/univ/ucsm0002/silo/4.10.2 source ~/.bash_profile Make sure correct modules are loaded Note: This may not always be necessary to do manually, but if you get errors such as mpi not loading correctly, load all the modules and see if that solves the problem. You do have to re-load the modules each time. Type command: module list a. You\u2019ll probably see something like below if you haven\u2019t made any changes: b. Currently Loaded Modules: 1. 1) ncarenv/1.3 2) intel/18.0.5 3) ncarcompilers/0.5.0 4) mpt/2.19 5) netcdf/4.6.3 Load the modules you need (example. below) c. aktriplett@cheyenne4:/glade/scratch/aktriplett> module load cmake 8. aktriplett@cheyenne4:/glade/scratch/aktriplett> module load nco e. aktriplett@cheyenne4:/glade/scratch/aktriplett> module load ncl f. aktriplett@cheyenne4:/glade/scratch/aktriplett> module load ncview g. aktriplett@cheyenne4:/glade/scratch/aktriplett> module load R h. aktriplett@cheyenne4:/glade/scratch/aktriplett> module list After you load the modules your list will look like this: Currently Loaded Modules: 1) ncarenv/1.3 3) ncarcompilers/0.5.0 5) netcdf/4.6.3 7) nco/4.7.9 9) ncview/2.1.7 2) intel/18.0.5 4) mpt/2.19 6) cmake/3.14.4 8) ncl/6.6.2 10) R/3.6.0 Job Scripts Cheyenne uses PBS for job scheduling. See general info Checking your Core Hour Usage Follow this link to the Systems Accounting Manager (SAM) system Sam.ucar.edu You can also look at this link for additional information about what is available on SAM https://www2.cisl.ucar.edu/user-support/systems-accounting-manager Log-in to the SAM system using your regular log-in info, you will have to authenticate with Duo push as well One logged in, you can go to the reports tab to track your usage activity, it will look something like this: Installing icommands on Cheyenne This is likely deprecated, use at your own risk (Ben West) It is possible to install icommands on Cheyenne without any additional privilege in your own directory. _Icommands if NOT natively installed on Cheyenne unlike UAHPC. _Detailed can be found here, https://wiki.cyverse.org/wiki/display/DS/Setting+Up+iCommands#SettingUpiCommands-co Download the installer to your local computer (links can be found in https://wiki.cyverse.org/wiki/display/DS/Setting+Up+iCommands#SettingUpiCommands-co ) and upload to your directory in Cheyenne. Download the ubuntu 14 version for unprivileged users In your Cheyenne directory with the installer, type or copy command: sh irods-icommands-4.1.10-ubuntu-14.installer Then you will see: Where would you like to install it? [/home/cyverse-user] Expanding contents under /home/cyverse-user. Updating .bashrc done! To make the changes take effect in the current shell, you will need to source your .bashrc file ( vim ~/.bash_profile ). The following lines should be added: # iRODS iCommands support export IRODS_PLUGINS_HOME=/glade/u/home/yourusername/icommands/plugins/ export PATH=/glade/u/home/yourusername/icommands:$PATH Open a new terminal window, if necessary, and run ienv to check that the new version was installed. initialise your account: _iinit_ [data.cyverse.org](http://data.cyverse.org/) Port: 1247 User: yourusername Zone: iplant Then enter your cyverse password Then you can start use icommands. Running with Python on Cheyenne General instructions of how to get running with python can be found at: https://www2.cisl.ucar.edu/resources/python-\u2013-ncar-package-library In order to install your own packages, you will have to create your own virtual environment after following the steps in the above page and then activate as follows: NOTE: You do not have permission to write in the ncar virtual env, so to install packages not included there, you have to create your own ncar_pylib -c 20201220 /glade/work/$USER/my_npl_clone ncar_pylib my_npl_clone pip --no-cache-dir install PACKAGE_NAME If your virtualenv stops working, you may have to delete it and create a new one, just follow the above steps again and you should be good Building a New Version of ParFlow on Cheyenne (08/2020) from MM at CSM. Use this for reference on module loading at the very least, Parflow blog can be used to suplement if any of the links to dependencies are deprecated. Adding specific ParFlow Builds to your bash profile. This is an example for a specific version, you need to add the path to the version you want to use after export: vim ~/.bash_profile . export PARFLOW_DIR=/glade/p/univ/ucsm0002/parflow_build/parflow #(this is the folder where you will put the new version of Parflow) export HYPRE_DIR=/glade/p/univ/ucsm0002/hypre/2.10.1 export SILO_DIR=/glade/p/univ/ucsm0002/silo/4.10.2 Build instructions Please load the following modules: module load impi module load intel/17.0.1 module load cmake To see which modules are available, use command module avail . To check which modules you have loaded, you can use module list . For me, this returns: Currently Loaded Modules: 1) ncarenv/1.3 3) ncarcompilers/0.5.0 5) cmake/3.14.4 7) nco/4.7.9 9) netcdf/4.6.3 2) intel/17.0.1 4) impi/2017.1.132 6) ncl/6.6.2 (H) 8) ncview/2.1.7 10) R/3.6.0 I was able to follow the typical installing directions for Silo and Hypre (from the parflow blog) and set them in my bash profile. Then just follow the cmake workflow for installing parflow: cd ~/glade/p/univ/ucsm0002/parflow_build (or wherever) git clone -b master http://github.com/parflow/parflow.git export PARFLOW_DIR=~//glade/p/univ/ucsm0002/parflow_build/parflow (or wherever. Add to bash_profile.) cd $PARFLOW_DIR mkdir build cd build cmake .. -DCMAKE_INSTALL_PREFIX=$PARFLOW_DIR -DPARFLOW_AMPS_LAYER=mpi1 -DPARFLOW_ENABLE_TIMING=TRUE -DHYPRE_ROOT=$HYPRE_DIR -DSILO_ROOT=$SILO_DIR -DPARFLOW_HAVE_CLM=ON -DPARFLOW_AMPS_SEQUENTIAL_IO=TRUE make make install","title":"Running on Cheyenne (NCAR)"},{"location":"how_to_notes/HPC_systems/cheyenne/#running-on-cheyenne-ncar","text":"**Amanda Triplett ** Original: 12/2019 Update: 08/2020","title":"Running on Cheyenne (NCAR)"},{"location":"how_to_notes/HPC_systems/cheyenne/#description","text":"** **This gives an overview with examples of how to remotely access HPC resources at NCAR (specifically Cheyenne), update your bash profile and modules, an example job_script with the commands Cheyenne needs to run, how to run and check your job as well as useful information to transfer and edit your files.","title":"Description"},{"location":"how_to_notes/HPC_systems/cheyenne/#software","text":"No specific software is needed, you do need an account with NCAR and an HPC time allotment in order to access resources. Everything can be done through your terminal.","title":"Software"},{"location":"how_to_notes/HPC_systems/cheyenne/#accessing-and-logging-in-to-cheyenne","text":"Go to your scratch folder 4. Every user starts with a scratch folder under their username, once you navigate here, you can create file directories just as you do on your own computer 4. cd /glade/scratch/\u201dyour_username\u201d 5. Mkdir \u201cnew_directory_name\u201d","title":"Accessing and logging in to Cheyenne"},{"location":"how_to_notes/HPC_systems/cheyenne/#updating-your-bash-profile-and-loading-modules-for-parflow","text":"Make any necessary updates to your bash profile, directions below are required if you want to run ParFlow - you should only need to do this once and make sure you go to your scratch folder. The first time you do it you will have to either source your bash profile or log out then back in. first vim ~/.bash_profile export PARFLOW_DIR=/glade/p/univ/ucsm0002/parflow3.7_0826_2020/parflow export HYPRE_DIR=/glade/p/univ/ucsm0002/hypre/2.10.1 export SILO_DIR=/glade/p/univ/ucsm0002/silo/4.10.2 source ~/.bash_profile Make sure correct modules are loaded Note: This may not always be necessary to do manually, but if you get errors such as mpi not loading correctly, load all the modules and see if that solves the problem. You do have to re-load the modules each time. Type command: module list a. You\u2019ll probably see something like below if you haven\u2019t made any changes: b. Currently Loaded Modules: 1. 1) ncarenv/1.3 2) intel/18.0.5 3) ncarcompilers/0.5.0 4) mpt/2.19 5) netcdf/4.6.3 Load the modules you need (example. below) c. aktriplett@cheyenne4:/glade/scratch/aktriplett> module load cmake 8. aktriplett@cheyenne4:/glade/scratch/aktriplett> module load nco e. aktriplett@cheyenne4:/glade/scratch/aktriplett> module load ncl f. aktriplett@cheyenne4:/glade/scratch/aktriplett> module load ncview g. aktriplett@cheyenne4:/glade/scratch/aktriplett> module load R h. aktriplett@cheyenne4:/glade/scratch/aktriplett> module list After you load the modules your list will look like this: Currently Loaded Modules: 1) ncarenv/1.3 3) ncarcompilers/0.5.0 5) netcdf/4.6.3 7) nco/4.7.9 9) ncview/2.1.7 2) intel/18.0.5 4) mpt/2.19 6) cmake/3.14.4 8) ncl/6.6.2 10) R/3.6.0","title":"Updating your bash profile and loading modules for Parflow"},{"location":"how_to_notes/HPC_systems/cheyenne/#job-scripts","text":"Cheyenne uses PBS for job scheduling. See general info","title":"Job Scripts"},{"location":"how_to_notes/HPC_systems/cheyenne/#checking-your-core-hour-usage","text":"Follow this link to the Systems Accounting Manager (SAM) system Sam.ucar.edu You can also look at this link for additional information about what is available on SAM https://www2.cisl.ucar.edu/user-support/systems-accounting-manager Log-in to the SAM system using your regular log-in info, you will have to authenticate with Duo push as well One logged in, you can go to the reports tab to track your usage activity, it will look something like this:","title":"Checking your Core Hour Usage"},{"location":"how_to_notes/HPC_systems/cheyenne/#installing-icommands-on-cheyenne","text":"This is likely deprecated, use at your own risk (Ben West) It is possible to install icommands on Cheyenne without any additional privilege in your own directory. _Icommands if NOT natively installed on Cheyenne unlike UAHPC. _Detailed can be found here, https://wiki.cyverse.org/wiki/display/DS/Setting+Up+iCommands#SettingUpiCommands-co Download the installer to your local computer (links can be found in https://wiki.cyverse.org/wiki/display/DS/Setting+Up+iCommands#SettingUpiCommands-co ) and upload to your directory in Cheyenne. Download the ubuntu 14 version for unprivileged users In your Cheyenne directory with the installer, type or copy command: sh irods-icommands-4.1.10-ubuntu-14.installer Then you will see: Where would you like to install it? [/home/cyverse-user] Expanding contents under /home/cyverse-user. Updating .bashrc done! To make the changes take effect in the current shell, you will need to source your .bashrc file ( vim ~/.bash_profile ). The following lines should be added: # iRODS iCommands support export IRODS_PLUGINS_HOME=/glade/u/home/yourusername/icommands/plugins/ export PATH=/glade/u/home/yourusername/icommands:$PATH Open a new terminal window, if necessary, and run ienv to check that the new version was installed. initialise your account: _iinit_ [data.cyverse.org](http://data.cyverse.org/) Port: 1247 User: yourusername Zone: iplant Then enter your cyverse password Then you can start use icommands.","title":"Installing icommands on Cheyenne"},{"location":"how_to_notes/HPC_systems/cheyenne/#running-with-python-on-cheyenne","text":"General instructions of how to get running with python can be found at: https://www2.cisl.ucar.edu/resources/python-\u2013-ncar-package-library In order to install your own packages, you will have to create your own virtual environment after following the steps in the above page and then activate as follows: NOTE: You do not have permission to write in the ncar virtual env, so to install packages not included there, you have to create your own ncar_pylib -c 20201220 /glade/work/$USER/my_npl_clone ncar_pylib my_npl_clone pip --no-cache-dir install PACKAGE_NAME If your virtualenv stops working, you may have to delete it and create a new one, just follow the above steps again and you should be good","title":"Running with Python on Cheyenne"},{"location":"how_to_notes/HPC_systems/cheyenne/#building-a-new-version-of-parflow-on-cheyenne-082020-from-mm-at-csm","text":"Use this for reference on module loading at the very least, Parflow blog can be used to suplement if any of the links to dependencies are deprecated. Adding specific ParFlow Builds to your bash profile. This is an example for a specific version, you need to add the path to the version you want to use after export: vim ~/.bash_profile . export PARFLOW_DIR=/glade/p/univ/ucsm0002/parflow_build/parflow #(this is the folder where you will put the new version of Parflow) export HYPRE_DIR=/glade/p/univ/ucsm0002/hypre/2.10.1 export SILO_DIR=/glade/p/univ/ucsm0002/silo/4.10.2 Build instructions Please load the following modules: module load impi module load intel/17.0.1 module load cmake To see which modules are available, use command module avail . To check which modules you have loaded, you can use module list . For me, this returns: Currently Loaded Modules: 1) ncarenv/1.3 3) ncarcompilers/0.5.0 5) cmake/3.14.4 7) nco/4.7.9 9) netcdf/4.6.3 2) intel/17.0.1 4) impi/2017.1.132 6) ncl/6.6.2 (H) 8) ncview/2.1.7 10) R/3.6.0 I was able to follow the typical installing directions for Silo and Hypre (from the parflow blog) and set them in my bash profile. Then just follow the cmake workflow for installing parflow: cd ~/glade/p/univ/ucsm0002/parflow_build (or wherever) git clone -b master http://github.com/parflow/parflow.git export PARFLOW_DIR=~//glade/p/univ/ucsm0002/parflow_build/parflow (or wherever. Add to bash_profile.) cd $PARFLOW_DIR mkdir build cd build cmake .. -DCMAKE_INSTALL_PREFIX=$PARFLOW_DIR -DPARFLOW_AMPS_LAYER=mpi1 -DPARFLOW_ENABLE_TIMING=TRUE -DHYPRE_ROOT=$HYPRE_DIR -DSILO_ROOT=$SILO_DIR -DPARFLOW_HAVE_CLM=ON -DPARFLOW_AMPS_SEQUENTIAL_IO=TRUE make make install","title":"Building a New Version of ParFlow on Cheyenne (08/2020) from MM at CSM."},{"location":"how_to_notes/HPC_systems/general_info/","text":"How to interact with HPC Once you have an account you can ssh into a terminal instance as such (example for Cheyenne) Open your terminal and type the following: 1. ssh -Y -l \u201cyour_username\u201d@cheyenne.ucar.edu (other HPCs would have a different address after the @) 1. _Note:_ Macs often need the -Y and -l commands 2. If it asks if you want to continue connecting, say yes 2. Enter your password and your 2FA token if necessary 3. You probably already set up duo when you set up your account, look on NCARs website for Cheyenne or call the help desk 3. You\u2019re logged in! For writing code it is recommended to use the in vscode. This will let you edit files on the machine via a vscode window on your machine. You can then run these files however your HPC requires. Where to do your runs If possible, it is preferred to setup your runs in a folder owned by the group you belong to, not your personal scratch directory. This is to make collaboration and the eventuality of handing off your model easier. It WILL be harder than you think to move from your personal storage to the group folder if you want to make that change down the line. Additionally, HPC systems often periodically wipe the data on them. It is advised to be familiar with what that policy is, and to run frequent backups using something like globus. Large file transfers The recommended tool for moving data in and out of NERSC for large transfers http://www.globus.org/ or http://globus.nersc.gov/ See also the group notes on Globus User scratch directory: /global/cscratch1/sd/yourusername Some HPCs do not have Globus available in which case you will need to determine what tool is best. I have found Globus to be the easiest option though when it is available (Ben West) If you cannot use these tools SCP or potentially Icommands can be used. Icommands instructions will need to be googled, the ones in this doc are deprecated. Conda It is recommended to use conda environments on HPC for both collaboration and to minimize the chance your build spontaneously breaks (no promises). You may have to check if your HPC requires a particular distribution or that you use a version it has already installed. Look at Conda Environments: conda env list Create Conda Environments: conda create --name -myenv Activate Conda Environments: conda activate and conda deactivate Alternatively one may use venv or virtualenv for your environment needs. Creating and monitoring jobs HPCs track the compute hours people use out of a pre-allocated amount. This means any serious compute (running models or analyzing results) will need to be done via the submission of a job. Most HPCs use a technology called slurm to manage any jobs (instances of compute you request). Below are some useful links, but also make sure to google \"slurm + name of your hpc\" because it may have minor differences between machines in terms of the settings available. https://docs.nersc.gov/jobs/#submitting-jobs https://docs.nersc.gov/jobs/#monitoring-jobs sbatch : submit a batch script sacct : display accounting data for jobs and job steps salloc : request nodes for an interactive batch session srun : launch parallel jobs scancel : delete a batch job sqs : NERSC custom queue display with job priority ranking info squeue : display info about jobs in the queue sinfo : view SLURM configuration about nodes and partitions scontrol : view and modify SLURM configuration and job state Some HPCs we use use something called PBS instead. See Tutorial Scribd article (with annoying ads but useful free content) and for an example job submission script: # Your job will use 1 node, 8 cores, and 48gb of memory total. #PBS -q standard #PBS -l select=1:ncpus=16:mem=48gb:pcmem=6gb ### Specify a name for the job #PBS -N test ### Specify the group name #PBS -W group_list=lecondon ### Walltime is how long your job will run #PBS -l walltime=00:50:00 ### Joins standard error and standard out #PBS -j oe cd /home/u18/lecondon/Test/washita/tcl_scripts tclsh Dist_Forcings.tcl tclsh LW_Test.tcl Debug queue The debug queue is a high-priority queue with a short runtime that is used to debug your runs before sending them to the standard or other queue. More information can be found here for UA HPC. All HPCs should have a debug queue available. We recommend you use the parflow blog if you can instead of this page The Parflow blog has install instructions for many systems. This is the recommended path for now as it is updated most often.","title":"General info"},{"location":"how_to_notes/HPC_systems/general_info/#how-to-interact-with-hpc","text":"Once you have an account you can ssh into a terminal instance as such (example for Cheyenne) Open your terminal and type the following: 1. ssh -Y -l \u201cyour_username\u201d@cheyenne.ucar.edu (other HPCs would have a different address after the @) 1. _Note:_ Macs often need the -Y and -l commands 2. If it asks if you want to continue connecting, say yes 2. Enter your password and your 2FA token if necessary 3. You probably already set up duo when you set up your account, look on NCARs website for Cheyenne or call the help desk 3. You\u2019re logged in! For writing code it is recommended to use the in vscode. This will let you edit files on the machine via a vscode window on your machine. You can then run these files however your HPC requires.","title":"How to interact with HPC"},{"location":"how_to_notes/HPC_systems/general_info/#where-to-do-your-runs","text":"If possible, it is preferred to setup your runs in a folder owned by the group you belong to, not your personal scratch directory. This is to make collaboration and the eventuality of handing off your model easier. It WILL be harder than you think to move from your personal storage to the group folder if you want to make that change down the line. Additionally, HPC systems often periodically wipe the data on them. It is advised to be familiar with what that policy is, and to run frequent backups using something like globus.","title":"Where to do your runs"},{"location":"how_to_notes/HPC_systems/general_info/#large-file-transfers","text":"The recommended tool for moving data in and out of NERSC for large transfers http://www.globus.org/ or http://globus.nersc.gov/ See also the group notes on Globus User scratch directory: /global/cscratch1/sd/yourusername Some HPCs do not have Globus available in which case you will need to determine what tool is best. I have found Globus to be the easiest option though when it is available (Ben West) If you cannot use these tools SCP or potentially Icommands can be used. Icommands instructions will need to be googled, the ones in this doc are deprecated.","title":"Large file transfers"},{"location":"how_to_notes/HPC_systems/general_info/#conda","text":"It is recommended to use conda environments on HPC for both collaboration and to minimize the chance your build spontaneously breaks (no promises). You may have to check if your HPC requires a particular distribution or that you use a version it has already installed. Look at Conda Environments: conda env list Create Conda Environments: conda create --name -myenv Activate Conda Environments: conda activate and conda deactivate Alternatively one may use venv or virtualenv for your environment needs.","title":"Conda"},{"location":"how_to_notes/HPC_systems/general_info/#creating-and-monitoring-jobs","text":"HPCs track the compute hours people use out of a pre-allocated amount. This means any serious compute (running models or analyzing results) will need to be done via the submission of a job. Most HPCs use a technology called slurm to manage any jobs (instances of compute you request). Below are some useful links, but also make sure to google \"slurm + name of your hpc\" because it may have minor differences between machines in terms of the settings available. https://docs.nersc.gov/jobs/#submitting-jobs https://docs.nersc.gov/jobs/#monitoring-jobs sbatch : submit a batch script sacct : display accounting data for jobs and job steps salloc : request nodes for an interactive batch session srun : launch parallel jobs scancel : delete a batch job sqs : NERSC custom queue display with job priority ranking info squeue : display info about jobs in the queue sinfo : view SLURM configuration about nodes and partitions scontrol : view and modify SLURM configuration and job state Some HPCs we use use something called PBS instead. See Tutorial Scribd article (with annoying ads but useful free content) and for an example job submission script: # Your job will use 1 node, 8 cores, and 48gb of memory total. #PBS -q standard #PBS -l select=1:ncpus=16:mem=48gb:pcmem=6gb ### Specify a name for the job #PBS -N test ### Specify the group name #PBS -W group_list=lecondon ### Walltime is how long your job will run #PBS -l walltime=00:50:00 ### Joins standard error and standard out #PBS -j oe cd /home/u18/lecondon/Test/washita/tcl_scripts tclsh Dist_Forcings.tcl tclsh LW_Test.tcl","title":"Creating and monitoring jobs"},{"location":"how_to_notes/HPC_systems/general_info/#debug-queue","text":"The debug queue is a high-priority queue with a short runtime that is used to debug your runs before sending them to the standard or other queue. More information can be found here for UA HPC. All HPCs should have a debug queue available.","title":"Debug queue"},{"location":"how_to_notes/HPC_systems/general_info/#we-recommend-you-use-the-parflow-blog-if-you-can-instead-of-this-page","text":"The Parflow blog has install instructions for many systems. This is the recommended path for now as it is updated most often.","title":"We recommend you use the parflow blog if you can instead of this page"},{"location":"how_to_notes/HPC_systems/parflow_install/","text":"ParFlow Install Laura Condon, 2/13/19 We recommend you use the parflow blog if you can instead of this page The blog has install instructions for many systems. If you can use a resource there to help you it is updated more frequently. This page has been left up in case any of the dependency links are platform dependent (as in if you hypotheticall need to download and build a specific version of MPI on Ocelot for Parflow), but it is only recommended to use it as reference to debug why your install is not working. Description A running set of notes on ParFlow installs for various platforms. Please update this with your experiences either by adding comments or inserting text. Please Add new installs to the top of this doc and use headings so that they can be included in the table of contents. **NOTE: If you are installing on a mac refer first to the ParFlow blog http://parflow.blogspot.com/ and look for the latest version in the compiling section which corresponds to your operating system. ** Links: You should also refer to the ParFlow blog for additional resources http://parflow.blogspot.com/ UA-HPC Ocelote Notes from Laura Fall 2018 Contacts Ric Anderson: ric@email.arizona.edu Chris Reidy: chrisreidy@email.arizona.edu Software install request form https://it.arizona.edu/service-request-forms They built Hypre and silo and these are available as modules Module commands https://docs.hpc.arizona.edu/display/UAHPC/Accessing+Software Module list (gives loaded modules) Module avail (gives list of all modules) Module show NAME (gives info on a specific module Building ParFlow ssh_ _username@hpc.arizona.edu Download ParFlow git clone https://github.com/parflow/parflow Rename this directory to whatever you want Setup environment vi ~/.bashrc Add the following lines to the user specified portion: module load gcc module load hypre module load silo export PARFLOW_DIR=/home/PATH_TO_YOUR_PARFLOW_DIR PATH=$PATH:$PARFLOW_DIR/bin source ~/.bashrc Build ParFlow cd pfsimulator ./configure --prefix=$PARFLOW_DIR --with-amps=mpi1 --with-clm --with-hypre=$HYPRE_BASE --with-silo=$SILO_BASE --with-amps-sequential-io make -j 14 make install Build PFtools cd ../pftools ./configure --prefix=$PARFLOW_DIR --with-amps=mpi1 --with-clm --with-hypre=$HYPRE_BASE --with-silo=$SILO_BASE --with-amps-sequential-io make -j 14 make install Run tests cd ../test make check MacOS Mojave These notes are from Nick Engdahl 2/12i/2019: This post will describe a \u201ctypical\u201d ParFlow installation, that is one that uses the most common options and configurations on macOS using the default bash shell. We\u2019ll primarily use the command line for this so go ahead and open a terminal window (if you don\u2019t know where terminal is use spotlight search). Preliminaries For commands entered into the terminal window I\u2019ll use the following convention where the command is preceded with \u201c >> \u201d for example: >> make {and here\u2019s an example comment/note you don\u2019t type} Anything that follows in curly brackets { } is just a note or comment and is NOT to be entered. OK, first thing\u2019s first. We need to define a few environment variables for the installation. Navigate to your home folder and create (or edit) your bash_profile: >> cd ~ >> vim .bash_profile In vim, hit \u201ci\u201d to enter insert mode and add the following lines, preferably at the bottom: export PARFLOW_DIR=~/ParF/parflow export HYPRE_DIR=~/ParF/hypre export SILO_DIR=~/ParF/silo export CC=clang export CXX=clang++ export FC=gfortran export F77=gfortran then press \u201cescape\u201d followed by \u201c:\u201d then \u201cx\u201d to save the changes and exit. To apply the changes: >> source .bash_profile This has now defined some environment variables and shortcuts we\u2019ll take advantage of later on. Next, we\u2019ll create a directory to hold all the various bits that comprise a typical installation. From your home directory, make a folder called \u201cParF\u201d and enter it: >> cd ~ {only necessary if you\u2019ve navigated out of home} >> mkdir ParF >> cd ParF There are 6 \u201cparts\u201d to these instructions and we\u2019re going to handle each separately: 1) install gcc/gfortran, 2) install cmake, 3) install OpenMPI, 4) Install Silo, 5) Install Hypre, 6) install ParFlow. Everything except step 6 are supporting libraries or programs that ParFlow needs for our typical installation. Step 1) Installing gfortran and gcc Truth be told we only really need gfortran but gcc is nice to have, and since they\u2019re often bundled it makes sense to knock both out. You could try building them from source code, but the easier way is to use a binary that is pre-built for MacOS. Those can be downloaded here: http://hpc.sourceforge.net The option you want is usually the first entry right below the word \u201cBinaries\u201d after the \u201cbig\u201d paragraph. Today, for me that is \u201cgcc-8.1-bin.tar.gz\u201d and clicking on that link put it in my \u201cDownloads\u201d folder. You don\u2019t need to bother with the \u201cgfortran\u201d link because it\u2019s already included in this one. In your terminal window, we first need to enable compiling programs on your mac. This is necessary after every major MacOS upgrade before you can compile new programs (old one will be fine though). >> sudo xcode-select --install A confirmation window will likely open and after you OK through all of that, we\u2019re done with that. You might also see a message saying command line tools are already installed, which is also fine. Next, navigate to wherever you put that download and extract it to /usr/local with: >> cd ~/Downloads {assuming your Download went here} >> sudo tar -xvf gcc-8.1-bin.tar -C / A lot of text will fly by then you\u2019ll get a command prompt. To make sure this has copied correctly, at the command prompt type: >> gfortran -v And you should get something like this: Using built-in specs. COLLECT_GCC=gfortran COLLECT_LTO_WRAPPER=/usr/local/libexec/gcc/x86_64-apple-darwin17.5.0/8.1.0/lto-wrapper Target: x86_64-apple-darwin17.5.0 Configured with: ../gcc-8.1.0/configure --enable-languages=c++,fortran Thread model: posix gcc version 8.1.0 (GCC) There is one last critically important step we need to take that was not necessary in the past. We need to install the package in MacOS that has some of the libraries we need in order to compile programs. These are located in \u201c/Library/Developer/CommandLineTools/Packages/\u201d and the easiest way to get there is to open a finder window, press command+shift+g, then paste in that path. There is only one package in this folder (macOS_SDK_headers_for_macOS_10.14.pkg), though its name might change, and to install it just double click on it and follow the instructions. I would also suggest you run \u201csudo --xcode-select --s /Library/Developer/CommandLineTools\u201d to make sure the system is pointing to the right version. Apple may be depreciating the package in future macOS releases, but we\u2019ll update you if/when that happens. Step 2) Installing cmake Past installation instructions have used the GNU autoconfig system. We\u2019ll still use that for some of the supporting components, but ParFlow itself is now built using the CMake interface. This isn\u2019t installed on your mac by default, so we\u2019ll start by adding it. We\u2019re going to download the latest version of the CMake source. There is also a GUI for CMake but we\u2019re not going to use it. Before proceeding, check here: https://cmake.org/download/ for the latest version. On the download page, go ahead and click on the link for the archive in the Unix/Linux source box and the download should complete shortly. Assuming the archive is in your ~/Downloads/ folder, we\u2019ll copy it into ParF, the expand the archive: As of this writing that file is \u201ccmake-3.14.0-rc1.tar.gz\u201d To move that download directly into the ParF folder, in your terminal window, be sure you\u2019re in the ParF directory, then type: >> cd ~/ParF/ >> cp ~/Downloads/cmake-3.14.0-rc1.tar . >> tar -xvf cmake-3.14.0-rc1.tar Note the period at the end of the second line. You will need to update the file name as newer versions of CMake are released. Now we\u2019ll enter the new directory and start building using cmake\u2019s helper: >> cd cmake-3.14.0-rc1 >> ./bootstrap {This will take a while} >> make {This will also take a while} >> sudo make install _ At the end of the make process you\u2019ll see percentage signs on the left edge of your screen and if it makes it to 100 without errors, you\u2019re all set. *From Jun: When installing cmake , on the step of ./bootstrap , sometimes it will pop up an error message that \u2018Unable to find any JVMs matching version\u2019, just follow the instruction to install the up-to-date JAVA\u2019 Step 3) Installing OpenMPI OpenMPI is the message passing interface we use for ParFlow (note that this is not the same as OpenMP). This is also what makes it parallel so we\u2019ll do this step next. Navigate back to your ParF directory: >> cd ~/ParF Next we\u2019re going to download openmpi. The simplest way to do that is from the command line using the \u201ccurl\u201d command, which is like \u201cwget\u201d for a mac. However, before doing so you might want to check to make sure you\u2019re downloading the most recent version at \u201chttps://www.open-mpi.org/software/ompi/\u201d You\u2019ll need to modify the version numbers if using a more recent release but otherwise it\u2019ll be just like this: >> curl \u201chttps://download.open-mpi.org/release/open-mpi/v4.0/openmpi-4.0.0.tar.gz\u201d -o \u201copenmpi-4.0.0.tar.gz\u201d (Note: When copying and pasting, sometimes quotes get messed up. If you get an error from that command about libcurl, just delete then re-add the quotation marks manually once you\u2019re in the terminal window.) Next, we\u2019ll decompress and extract the archive in one step: >> gunzip -c openmpi-4.0.0.tar.gz | tar xf - Next, we\u2019ll configure the installation package for our system. Navigate into the newly created openmpi-4.0.0 folder and type: >> ./configure --prefix=/usr/local CC=/usr/bin/clang CXX=/usr/bin/clang++ Those CC and CXX arguments force the computer to use a specific compiler, which is clang. On most systems this should install fine without those, but it doesn\u2019t hurt to have them. After you hit return, lots of stuff will fly by your screen for a long time. Once that\u2019s done, type: >> sudo make all install which will take forever and a decade before it completes, but then that\u2019s it; OpenMPI is now installed. If you want to check, type \u201cmpirun\u201d at the command line, but not much will happen since it can\u2019t find anything to do. Important: Remember that sudo is the \u201cnuclear option\u201d on unix-esque computers and should only be used when absolutely necessary; the install path for OpenMPI is inside a protected folder so we need to invoke sudo in this case, otherwise the installation can\u2019t go to the default. You could specify an alternate location, but since most applications that might use OpenMPI will look here, we might as well. Step 4) Installing Silo Silo is a file format that is used as the default for viewing ParFlow outputs, but we also now have other options available. Many of the test cases use it so we always recommend installing it. The latest version of silo can be found at: https://wci.llnl.gov/simulation/computer-codes/silo/downloads You could download the archive in a web browser and move it but instead I\u2019m going to use curl again. Navigate to the ParF folder and the commands are: >> cd ~/ParF >> curl \u201chttps://wci.llnl.gov/content/assets/docs/simulation/computer-codes/silo/silo-4.10.2/silo-4.10.2.tar.gz\u201d -o \u201csilo-4.10.2.tar.gz\u201d Mimicking the process we used for OpenMPI, to expand and extract the archive type: >> gunzip -c silo-4.10.2.tar.gz | tar xf - Now we\u2019ll rename the newly expanded directory to match our environment variable SILO_DIR: >> mv silo-4.10.2 silo Now we\u2019ll navigate into the new directory, configure, and install: >> cd silo >> ./configure --disable-silex >> make install >> cd .. And that completes the installation. Note that if your version of Silo differs, you\u2019ll need to update the appropriate file/directory names environment variable we set to reflect your installation path. Step 5) Installing the HYPRE library HYPRE is a library of numerical solvers for high-performance computing applications and ParFlow hooks into it for some of its functionality. As before the simplest way to get this is curl. There were some issues with hypre version 10 and ParFlow but the current release (2.11.2) or newer should work just fine. Installing this is nothing more than a minor variation of what we\u2019ve already done: >> cd ~/ParF >> curl \u201chttps://computating.llnl.gov/projects/hypre-scalable-linear-solvers-multigrid-methods/download/hypre-2.11.2.tar.gz\u201d -o \u201chypre-2.11.2.tar.gz\u201d ( from Jen the above is the correct url for the hypre installation) >> gunzip -c hypre-2.11.2.tar.gz | tar xf - mv hypre-2.11.2 hypre` (That would vary if the version number changes, and your HYPRE_DIR variable would need to reflect your version/directory names). Now onto the setup. >> cd ~/ParF/ hypre/src >> ./configure --prefix=$HYPRE_DIR >> make install From Jen: If make install is not working and you receive a Fatal error mpi.h is not found: go into your bash_profile and change the two CC and CXX lines to the normal mpi complier export PARFLOW_DIR=~/ParF/parflow export HYPRE_DIR=~/ParF/hypre export SILO_DIR=~/ParF/silo export CC=mpicc export CXX=mpic++ export FC=gfortran export F77=gfortran Once done: Recompile via .configure --prefix=$HYPRE_DIR Then make install This should correct the previous error and allow you to install This installation also takes a while to install, but it\u2019s usually not as long as OpenMPI. You\u2019re likely to see a lot of warnings about \u201ccould not find any symbols\u201d but as long as there are no errors you should be able to move on to ParFlow. Step 6) Installing ParFlow This step is where we depart significantly from the previous installation instructions, but in a good way. The ParFlow build process is now more streamlined because it uses CMake. Instead of separate builds for pfsimulator (the model itself) and pftools (the controlling interface), there is one unified build process. There are a lot of different setup options for ParFlow but for our \u201ctypical\u201d setup we\u2019re assuming you want to run in parallel, you might want to run CLM, you have Silo and Hypre, and a few other little things. First, we need to download the code. ParFlow is hosted on GitHub, so to obtain the code: >> cd ~/ParF >> git clone https://github.com/parflow/parflow.git --branch master --single-branch_ which will create a folder called parflow in ParF and now you\u2019ve got the source code. You may want to rename the folder (I like to use the date I downloaded it so /parflow.20190209 for example) but you\u2019ll need to update your PARFLOW_DIR too if you change that. I\u2019m going to proceed assuming you have not renamed it. Next, we\u2019ll create a directory for the incremental files of the build process called \u201cbuild\u201d >> mkdir build >> cd build The next step will tell CMake how to configure ParFlow using the typical options (copy and paste will be your friend on this): >> cmake ../parflow -DCMAKE_INSTALL_PREFIX=$PARFLOW_DIR -DHYPRE_ROOT=$HYPRE_DIR -DPARFLOW_AMPS_LAYER=mpi1 -DPARFLOW_AMPS_SEQUENTIAL_IO=true -DPARFLOW_ENABLE_TIMING=true -DSILO_ROOT=$SILO_DIR -DPARFLOW_HAVE_CLM=ON Note that if your parflow source tree is NOT in a folder called parflow, you\u2019ll need to change that right after \u201ccmake\u201d in the above command. The manual provides a few extra details on how you can customize an installation using the \u201cccmake\u201d interface but we don\u2019t need that now. Next type: >> make and lots of stuff will happen. If the last two lines resemble: [100%] Linking C shared library libpftools.dylib [100%] Built target pftools Then you\u2019re ready for the last command: >> make install The difference is that \u201cmake\u201d builds everything in our temporary structure (the reason we created that build directory in the first), and \u201cmake install\u201d copies the final build into our installation location. On a few computers we\u2019ve found that after the \u201cmake\u201d command you might see something like this: collect2: error: ld returned 1 exit status make[2]: *** [pftools/libpftools.dylib] Error 1 make[1]: *** [pftools/CMakeFiles/pftools.dir/all] Error 2 make: *** [all] Error 2 If that\u2019s the case, just back up and add \u201c-DCMAKE_SHARED_LINKER_FLAGS=-ltcl8.5\u201d to the end of that list for the \u201ccmake\u201d command, the \u201cmake\u201d again and you should be all sorted out. And that\u2019s it for the installation. The last step is to test the installation. Navigate to: >> cd ~/ParF/parflow/test {Or wherever you installed to} Then type: >> make check A lot of things will fly by as ParFlow verifies that it is working correctly. Most should pass, but some will likely fail if you have fewer processors than are being tested; for example, some of the tests need nine processors and these will fail on a system with only 4. You\u2019ll see a report at the end and as long as most pass, you\u2019re all done and ParFlow has been installed successfully. Step 7) Some troubleshooting tips The single most common reason steps of the installation don\u2019t go well is not being in the correct path. You can always verify your location in the directory tree using the \u201cpwd\u201d command and each step tells you where you should be before you begin entering commands. You mac is also case-sensitive so pay attention to capitalization. That also tends to be a wrench in the gears of the setup process. If you prefer to use the c-shell instead of the bash-shell most of the steps are the same but your environment variables are defined differently. Fortunately, the setup example in the ParFlow manual outlines the c-shell variations of all the steps we covered here. Lastly, if you have MacPorts or any other \u201chelper packages\u201d your system paths may have been altered from the defaults. Defining the environment variables at the bottom of your .bash_profile should take care of this. If you\u2019ve installed mpi properly but got errors with mpi, one solution can be turn off WIFI and keep WIFI off when running ParFLow. Step 8) What to do next See that wasn\u2019t nearly as bad as you thought it was going to be, so now what? Well, you could start getting to work with ParFlow (which is totally what you should do if you\u2019re a grad student), but maybe go outside, enjoy some fresh air, and see how things are today first. Maybe take a break and have a tasty beverage of your choosing while you ponder the intricacies of the scientific awesomeness you\u2019re going to unleash upon the world using ParFlow. MacOS High Sierra Notes from Laura: 9/11/2018 Summary: Successful install: With GCC 8.1.0, MPI 3.1.2, Hypre 2.9.0b, Slio 4.10.2 Only modification from PF Blog is that I compiled MPI with clang per Nick\u2019s instructions (see the configure line below). Also it didn\u2019t work when I ran tests with WiFI on when I was at school (MPI error) but it worked once I turned it off. Edited the bash profile per the instructions in the Yosemite blog post: http://parflow.blogspot.com/2014/10/installing-parflow-in-os-x-yosemite.html Installed the latest GCC per blog instructions: http://hpc.sourceforge.net/ After this it still wasn\u2019t pointing to the correct gcc but this was fixed by a restart dhcp-10-134-192-214:~ laura$ gcc --version gcc (GCC) 8.1.0 Copyright (C) 2018 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. dhcp-10-134-192-214:~ laura$ gfortran --version GNU Fortran (GCC) 8.1.0 Copyright (C) 2018 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. Next I got the latest MPI: https://www.open-mpi.org/software/ompi/v3.1/ tar -xvf openmpi-3.1.2.tar.gz cd openmpi-3.1.2 ./configure --prefix=/usr/local/ CC=/usr/bin/clang CXX=/usr/bin/clang cd openmpi-3.1.2 sudo make all install ompi_info mpiexec \u2014version mpirun \u2014version Get Parflow: git clone https://github.com/parflow/parflow mv parflow/ parflow.git Get Silo: https://wci.llnl.gov/simulation/computer-codes/silo/downloads Follow instructions from ParFlow Blog to build: > cd silo > ./configure --disable-silex > make install > cd .. Get Hypre 2.9.0b: https://computation.llnl.gov/projects/hypre-scalable-linear-solvers-multigrid-methods/software Follow instructions from ParFlow Blog to build: > tar -xvf hypre-2.9.0b.tar.gz > mv hypre-2.9.0b hypre > cd hypre/src > ./configure --prefix=$HYPRE_DIR > make install Followed steps 7 and 8 from the blog to install par flow ( http://parflow.blogspot.com/2014/10/installing-parflow-in-os-x-yosemite.html ) All tests were failing with an MPI error when I had wifi turned on but when I turned it off they all pass Other People\u2019s Notes on High Seirra: From Nick: For MPI: Version 3.0.0 Two catches to this though. 1) I had to use clang, and 2) I specified the path and compilers manually. Here\u2019s the exact line I used:./configure --prefix=/usr/local/ CC=/usr/bin/clang CXX=/usr/bin/clang(FYI you can verify your MPI version with \u201compi_info\u201d at the command line and the version will be at the top) Next Hypre/Silo: It\u2019s Hypre 2.9.0b and Silo 2.9.1. I haven\u2019t tried any of the other versions lately, BUT these were installed with gcc, looks like v4.9.0 according to the config log And PF was built with the same GCC and gfortran version used for hypre. I had no luck getting PF to build with clang so, annoyingly, it\u2019s exactly the opposite of openMPI. From Reed: /Users/reed/parflow/libraries> gcc \u2014version gcc (GCC) 5.0.0 20141005 (experimental) Copyright (C) 2014 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. /Users/reed/parflow/libraries> gfortran \u2014version GNU Fortran (GCC) 5.0.0 20141005 (experimental) Copyright (C) 2014 Free Software Foundation, Inc. GNU Fortran comes with NO WARRANTY, to the extent permitted by law. You may redistribute copies of GNU Fortran under the terms of the GNU General Public License. For more information about these matters, see the file named COPYING PFNetCDF on TACC Stampede2 Katie Markovich, March 28, 2019 Home directory = /home1/06175/khm293 [make this your home directory!] .**bashrc** module load git module load gcc export CC=gcc export CXX=g++ export FC=gfortran export F77=gfortran export NCDIR=/home1/06175/khm293/ParF/netcdf4 export H5DIR=/home1/06175/khm293/ParF/hdf5 export MPI_DIR=/home1/06175/khm293/ParF/OMP-4.0.0 export MPI_RUN_DIR=/home1/06175/khm293/ParF/OMP-4.0.0/bin export HYPRE_DIR=/home1/06175/khm293/ParF/hypre export SILO_DIR=/home1/06175/khm293/ParF/silo export PARFLOW_DIR=/home1/06175/khm293/ParF/parflow PATH=$PATH:$PARFLOW_DIR/bin **hdf5** Download hdf5 here: https://www.hdfgroup.org/downloads/ >> cd ~ >> mkdir ParF >> cd ParF >>mkdir hdf5 >>exportH5DIR=${HOME}/ParF/hdf5 >>gunzip hdf5-1.10.4.tar.gz >>tar -xvf hdf5-1.10.4.tar >>cd hdf5-1.10.4/ >>CC=mpicc ./configure --enable-parallel --prefix=${H5DIR} >>make >>make install >>cd .. netCDF4 Download netCDF4 here: https://www.unidata.ucar.edu/downloads/netcdf/index.jsp >>mkdir ${HOME}/ParF/netcdf4 >>exportNCDIR=${HOME}/ParF/netcdf4 >>gunzip netcdf-4.6.2.tar.gz >>tar -xvf netcdf-4.6.2.tar >>cd netcdf-4.6.2/ >>CC=mpicc CPPFLAGS=-I${H5DIR}/include LDFLAGS=-L${H5DIR}/lib \\ ./configure --disable-shared--disable-dap--enable-parallel-tests --prefix=${NCDIR} >>make >>make install From Nick\u2019s post: openMPI >> cd ~ >> cd ParF >> mkdir OMPI-4.0.0 >> curl \"https://download.open-mpi.org/release/open-mpi/v4.0/openmpi-4.0.0.tar.gz\" -o \"openmpi-4.0.0.tar.gz\" >> gunzip -c openmpi-4.0.0.tar.gz | tar xf - >> cd openmpi-4.0.0 >> ./configure --prefix=$MPI_DIR >> make >> make install Silo >> cd ~/ParF >> curl \"https://wci.llnl.gov/content/assets/docs/simulation/computer-codes/silo/silo-4.10.2/silo-4.10.2.tar.gz\" -o \"silo-4.10.2.tar.gz\" >> gunzip -c silo-4.10.2.tar.gz | tar xf - >> mv silo-4.10.2 silo >> cd silo >> ./configure --disable-silex >> make install Hypre >> cd .. >> cd ~/ParF >> curl \"https://computation.llnl.gov/projects/hypre-scalable-linear-solvers-multigrid-methods/download/hypre-2.11.2.tar.gz\" -o \"hypre-2.11.2.tar.gz\" >> gunzip -c hypre-2.11.2.tar.gz | tar xf \u2013 >> mv hypre-2.11.2 hypre >> cd ~/ParF/ hypre/src >> ./configure --prefix=$HYPRE_DIR >> make install Parflow-NetCDF >> cd ~/ParF >> git clone https://github.com/parflow/parflow.git --branch master --single-branch >> mkdir build >> cd build >>cmake ../parflow -DCMAKE_INSTALL_PREFIX=$PARFLOW_DIR -DHYPRE_ROOT=$HYPRE_DIR -DPARFLOW_AMPS_LAYER=mpi1 -DPARFLOW_AMPS_SEQUENTIAL_IO=true -DPARFLOW_ENABLE_TIMING=true -DSILO_ROOT=$SILO_DIR -DPARFLOW_HAVE_CLM=ON -DNETCDF_DIR=$NCDIR -DHDF5_ROOT=$H5DIR","title":"ParFlow Install"},{"location":"how_to_notes/HPC_systems/parflow_install/#parflow-install","text":"Laura Condon, 2/13/19","title":"ParFlow Install"},{"location":"how_to_notes/HPC_systems/parflow_install/#we-recommend-you-use-the-parflow-blog-if-you-can-instead-of-this-page","text":"The blog has install instructions for many systems. If you can use a resource there to help you it is updated more frequently. This page has been left up in case any of the dependency links are platform dependent (as in if you hypotheticall need to download and build a specific version of MPI on Ocelot for Parflow), but it is only recommended to use it as reference to debug why your install is not working.","title":"We recommend you use the parflow blog if you can instead of this page"},{"location":"how_to_notes/HPC_systems/parflow_install/#description","text":"A running set of notes on ParFlow installs for various platforms. Please update this with your experiences either by adding comments or inserting text. Please Add new installs to the top of this doc and use headings so that they can be included in the table of contents. **NOTE: If you are installing on a mac refer first to the ParFlow blog http://parflow.blogspot.com/ and look for the latest version in the compiling section which corresponds to your operating system. ** Links: You should also refer to the ParFlow blog for additional resources http://parflow.blogspot.com/","title":"Description"},{"location":"how_to_notes/HPC_systems/parflow_install/#ua-hpc-ocelote","text":"Notes from Laura Fall 2018","title":"UA-HPC Ocelote"},{"location":"how_to_notes/HPC_systems/parflow_install/#contacts","text":"Ric Anderson: ric@email.arizona.edu Chris Reidy: chrisreidy@email.arizona.edu","title":"Contacts"},{"location":"how_to_notes/HPC_systems/parflow_install/#software-install-request-form","text":"https://it.arizona.edu/service-request-forms They built Hypre and silo and these are available as modules","title":"Software install request form"},{"location":"how_to_notes/HPC_systems/parflow_install/#module-commands","text":"https://docs.hpc.arizona.edu/display/UAHPC/Accessing+Software Module list (gives loaded modules) Module avail (gives list of all modules) Module show NAME (gives info on a specific module","title":"Module commands"},{"location":"how_to_notes/HPC_systems/parflow_install/#building-parflow","text":"ssh_ _username@hpc.arizona.edu Download ParFlow git clone https://github.com/parflow/parflow Rename this directory to whatever you want Setup environment vi ~/.bashrc Add the following lines to the user specified portion: module load gcc module load hypre module load silo export PARFLOW_DIR=/home/PATH_TO_YOUR_PARFLOW_DIR PATH=$PATH:$PARFLOW_DIR/bin source ~/.bashrc","title":"Building ParFlow"},{"location":"how_to_notes/HPC_systems/parflow_install/#build-parflow","text":"cd pfsimulator ./configure --prefix=$PARFLOW_DIR --with-amps=mpi1 --with-clm --with-hypre=$HYPRE_BASE --with-silo=$SILO_BASE --with-amps-sequential-io make -j 14 make install Build PFtools cd ../pftools ./configure --prefix=$PARFLOW_DIR --with-amps=mpi1 --with-clm --with-hypre=$HYPRE_BASE --with-silo=$SILO_BASE --with-amps-sequential-io make -j 14 make install Run tests cd ../test make check","title":"Build ParFlow"},{"location":"how_to_notes/HPC_systems/parflow_install/#macos-mojave","text":"These notes are from Nick Engdahl 2/12i/2019: This post will describe a \u201ctypical\u201d ParFlow installation, that is one that uses the most common options and configurations on macOS using the default bash shell. We\u2019ll primarily use the command line for this so go ahead and open a terminal window (if you don\u2019t know where terminal is use spotlight search).","title":"MacOS Mojave"},{"location":"how_to_notes/HPC_systems/parflow_install/#preliminaries","text":"For commands entered into the terminal window I\u2019ll use the following convention where the command is preceded with \u201c >> \u201d for example: >> make {and here\u2019s an example comment/note you don\u2019t type} Anything that follows in curly brackets { } is just a note or comment and is NOT to be entered. OK, first thing\u2019s first. We need to define a few environment variables for the installation. Navigate to your home folder and create (or edit) your bash_profile: >> cd ~ >> vim .bash_profile In vim, hit \u201ci\u201d to enter insert mode and add the following lines, preferably at the bottom: export PARFLOW_DIR=~/ParF/parflow export HYPRE_DIR=~/ParF/hypre export SILO_DIR=~/ParF/silo export CC=clang export CXX=clang++ export FC=gfortran export F77=gfortran then press \u201cescape\u201d followed by \u201c:\u201d then \u201cx\u201d to save the changes and exit. To apply the changes: >> source .bash_profile This has now defined some environment variables and shortcuts we\u2019ll take advantage of later on. Next, we\u2019ll create a directory to hold all the various bits that comprise a typical installation. From your home directory, make a folder called \u201cParF\u201d and enter it: >> cd ~ {only necessary if you\u2019ve navigated out of home} >> mkdir ParF >> cd ParF There are 6 \u201cparts\u201d to these instructions and we\u2019re going to handle each separately: 1) install gcc/gfortran, 2) install cmake, 3) install OpenMPI, 4) Install Silo, 5) Install Hypre, 6) install ParFlow. Everything except step 6 are supporting libraries or programs that ParFlow needs for our typical installation. Step 1) Installing gfortran and gcc Truth be told we only really need gfortran but gcc is nice to have, and since they\u2019re often bundled it makes sense to knock both out. You could try building them from source code, but the easier way is to use a binary that is pre-built for MacOS. Those can be downloaded here: http://hpc.sourceforge.net The option you want is usually the first entry right below the word \u201cBinaries\u201d after the \u201cbig\u201d paragraph. Today, for me that is \u201cgcc-8.1-bin.tar.gz\u201d and clicking on that link put it in my \u201cDownloads\u201d folder. You don\u2019t need to bother with the \u201cgfortran\u201d link because it\u2019s already included in this one. In your terminal window, we first need to enable compiling programs on your mac. This is necessary after every major MacOS upgrade before you can compile new programs (old one will be fine though). >> sudo xcode-select --install A confirmation window will likely open and after you OK through all of that, we\u2019re done with that. You might also see a message saying command line tools are already installed, which is also fine. Next, navigate to wherever you put that download and extract it to /usr/local with: >> cd ~/Downloads {assuming your Download went here} >> sudo tar -xvf gcc-8.1-bin.tar -C / A lot of text will fly by then you\u2019ll get a command prompt. To make sure this has copied correctly, at the command prompt type: >> gfortran -v And you should get something like this: Using built-in specs. COLLECT_GCC=gfortran COLLECT_LTO_WRAPPER=/usr/local/libexec/gcc/x86_64-apple-darwin17.5.0/8.1.0/lto-wrapper Target: x86_64-apple-darwin17.5.0 Configured with: ../gcc-8.1.0/configure --enable-languages=c++,fortran Thread model: posix gcc version 8.1.0 (GCC) There is one last critically important step we need to take that was not necessary in the past. We need to install the package in MacOS that has some of the libraries we need in order to compile programs. These are located in \u201c/Library/Developer/CommandLineTools/Packages/\u201d and the easiest way to get there is to open a finder window, press command+shift+g, then paste in that path. There is only one package in this folder (macOS_SDK_headers_for_macOS_10.14.pkg), though its name might change, and to install it just double click on it and follow the instructions. I would also suggest you run \u201csudo --xcode-select --s /Library/Developer/CommandLineTools\u201d to make sure the system is pointing to the right version. Apple may be depreciating the package in future macOS releases, but we\u2019ll update you if/when that happens. Step 2) Installing cmake Past installation instructions have used the GNU autoconfig system. We\u2019ll still use that for some of the supporting components, but ParFlow itself is now built using the CMake interface. This isn\u2019t installed on your mac by default, so we\u2019ll start by adding it. We\u2019re going to download the latest version of the CMake source. There is also a GUI for CMake but we\u2019re not going to use it. Before proceeding, check here: https://cmake.org/download/ for the latest version. On the download page, go ahead and click on the link for the archive in the Unix/Linux source box and the download should complete shortly. Assuming the archive is in your ~/Downloads/ folder, we\u2019ll copy it into ParF, the expand the archive: As of this writing that file is \u201ccmake-3.14.0-rc1.tar.gz\u201d To move that download directly into the ParF folder, in your terminal window, be sure you\u2019re in the ParF directory, then type: >> cd ~/ParF/ >> cp ~/Downloads/cmake-3.14.0-rc1.tar . >> tar -xvf cmake-3.14.0-rc1.tar Note the period at the end of the second line. You will need to update the file name as newer versions of CMake are released. Now we\u2019ll enter the new directory and start building using cmake\u2019s helper: >> cd cmake-3.14.0-rc1 >> ./bootstrap {This will take a while} >> make {This will also take a while} >> sudo make install _ At the end of the make process you\u2019ll see percentage signs on the left edge of your screen and if it makes it to 100 without errors, you\u2019re all set. *From Jun: When installing cmake , on the step of ./bootstrap , sometimes it will pop up an error message that \u2018Unable to find any JVMs matching version\u2019, just follow the instruction to install the up-to-date JAVA\u2019 Step 3) Installing OpenMPI OpenMPI is the message passing interface we use for ParFlow (note that this is not the same as OpenMP). This is also what makes it parallel so we\u2019ll do this step next. Navigate back to your ParF directory: >> cd ~/ParF Next we\u2019re going to download openmpi. The simplest way to do that is from the command line using the \u201ccurl\u201d command, which is like \u201cwget\u201d for a mac. However, before doing so you might want to check to make sure you\u2019re downloading the most recent version at \u201chttps://www.open-mpi.org/software/ompi/\u201d You\u2019ll need to modify the version numbers if using a more recent release but otherwise it\u2019ll be just like this: >> curl \u201chttps://download.open-mpi.org/release/open-mpi/v4.0/openmpi-4.0.0.tar.gz\u201d -o \u201copenmpi-4.0.0.tar.gz\u201d (Note: When copying and pasting, sometimes quotes get messed up. If you get an error from that command about libcurl, just delete then re-add the quotation marks manually once you\u2019re in the terminal window.) Next, we\u2019ll decompress and extract the archive in one step: >> gunzip -c openmpi-4.0.0.tar.gz | tar xf - Next, we\u2019ll configure the installation package for our system. Navigate into the newly created openmpi-4.0.0 folder and type: >> ./configure --prefix=/usr/local CC=/usr/bin/clang CXX=/usr/bin/clang++ Those CC and CXX arguments force the computer to use a specific compiler, which is clang. On most systems this should install fine without those, but it doesn\u2019t hurt to have them. After you hit return, lots of stuff will fly by your screen for a long time. Once that\u2019s done, type: >> sudo make all install which will take forever and a decade before it completes, but then that\u2019s it; OpenMPI is now installed. If you want to check, type \u201cmpirun\u201d at the command line, but not much will happen since it can\u2019t find anything to do. Important: Remember that sudo is the \u201cnuclear option\u201d on unix-esque computers and should only be used when absolutely necessary; the install path for OpenMPI is inside a protected folder so we need to invoke sudo in this case, otherwise the installation can\u2019t go to the default. You could specify an alternate location, but since most applications that might use OpenMPI will look here, we might as well. Step 4) Installing Silo Silo is a file format that is used as the default for viewing ParFlow outputs, but we also now have other options available. Many of the test cases use it so we always recommend installing it. The latest version of silo can be found at: https://wci.llnl.gov/simulation/computer-codes/silo/downloads You could download the archive in a web browser and move it but instead I\u2019m going to use curl again. Navigate to the ParF folder and the commands are: >> cd ~/ParF >> curl \u201chttps://wci.llnl.gov/content/assets/docs/simulation/computer-codes/silo/silo-4.10.2/silo-4.10.2.tar.gz\u201d -o \u201csilo-4.10.2.tar.gz\u201d Mimicking the process we used for OpenMPI, to expand and extract the archive type: >> gunzip -c silo-4.10.2.tar.gz | tar xf - Now we\u2019ll rename the newly expanded directory to match our environment variable SILO_DIR: >> mv silo-4.10.2 silo Now we\u2019ll navigate into the new directory, configure, and install: >> cd silo >> ./configure --disable-silex >> make install >> cd .. And that completes the installation. Note that if your version of Silo differs, you\u2019ll need to update the appropriate file/directory names environment variable we set to reflect your installation path. Step 5) Installing the HYPRE library HYPRE is a library of numerical solvers for high-performance computing applications and ParFlow hooks into it for some of its functionality. As before the simplest way to get this is curl. There were some issues with hypre version 10 and ParFlow but the current release (2.11.2) or newer should work just fine. Installing this is nothing more than a minor variation of what we\u2019ve already done: >> cd ~/ParF >> curl \u201chttps://computating.llnl.gov/projects/hypre-scalable-linear-solvers-multigrid-methods/download/hypre-2.11.2.tar.gz\u201d -o \u201chypre-2.11.2.tar.gz\u201d ( from Jen the above is the correct url for the hypre installation) >> gunzip -c hypre-2.11.2.tar.gz | tar xf - mv hypre-2.11.2 hypre` (That would vary if the version number changes, and your HYPRE_DIR variable would need to reflect your version/directory names). Now onto the setup. >> cd ~/ParF/ hypre/src >> ./configure --prefix=$HYPRE_DIR >> make install From Jen: If make install is not working and you receive a Fatal error mpi.h is not found: go into your bash_profile and change the two CC and CXX lines to the normal mpi complier export PARFLOW_DIR=~/ParF/parflow export HYPRE_DIR=~/ParF/hypre export SILO_DIR=~/ParF/silo export CC=mpicc export CXX=mpic++ export FC=gfortran export F77=gfortran Once done: Recompile via .configure --prefix=$HYPRE_DIR Then make install This should correct the previous error and allow you to install This installation also takes a while to install, but it\u2019s usually not as long as OpenMPI. You\u2019re likely to see a lot of warnings about \u201ccould not find any symbols\u201d but as long as there are no errors you should be able to move on to ParFlow. Step 6) Installing ParFlow This step is where we depart significantly from the previous installation instructions, but in a good way. The ParFlow build process is now more streamlined because it uses CMake. Instead of separate builds for pfsimulator (the model itself) and pftools (the controlling interface), there is one unified build process. There are a lot of different setup options for ParFlow but for our \u201ctypical\u201d setup we\u2019re assuming you want to run in parallel, you might want to run CLM, you have Silo and Hypre, and a few other little things. First, we need to download the code. ParFlow is hosted on GitHub, so to obtain the code: >> cd ~/ParF >> git clone https://github.com/parflow/parflow.git --branch master --single-branch_ which will create a folder called parflow in ParF and now you\u2019ve got the source code. You may want to rename the folder (I like to use the date I downloaded it so /parflow.20190209 for example) but you\u2019ll need to update your PARFLOW_DIR too if you change that. I\u2019m going to proceed assuming you have not renamed it. Next, we\u2019ll create a directory for the incremental files of the build process called \u201cbuild\u201d >> mkdir build >> cd build The next step will tell CMake how to configure ParFlow using the typical options (copy and paste will be your friend on this): >> cmake ../parflow -DCMAKE_INSTALL_PREFIX=$PARFLOW_DIR -DHYPRE_ROOT=$HYPRE_DIR -DPARFLOW_AMPS_LAYER=mpi1 -DPARFLOW_AMPS_SEQUENTIAL_IO=true -DPARFLOW_ENABLE_TIMING=true -DSILO_ROOT=$SILO_DIR -DPARFLOW_HAVE_CLM=ON Note that if your parflow source tree is NOT in a folder called parflow, you\u2019ll need to change that right after \u201ccmake\u201d in the above command. The manual provides a few extra details on how you can customize an installation using the \u201cccmake\u201d interface but we don\u2019t need that now. Next type: >> make and lots of stuff will happen. If the last two lines resemble: [100%] Linking C shared library libpftools.dylib [100%] Built target pftools Then you\u2019re ready for the last command: >> make install The difference is that \u201cmake\u201d builds everything in our temporary structure (the reason we created that build directory in the first), and \u201cmake install\u201d copies the final build into our installation location. On a few computers we\u2019ve found that after the \u201cmake\u201d command you might see something like this: collect2: error: ld returned 1 exit status make[2]: *** [pftools/libpftools.dylib] Error 1 make[1]: *** [pftools/CMakeFiles/pftools.dir/all] Error 2 make: *** [all] Error 2 If that\u2019s the case, just back up and add \u201c-DCMAKE_SHARED_LINKER_FLAGS=-ltcl8.5\u201d to the end of that list for the \u201ccmake\u201d command, the \u201cmake\u201d again and you should be all sorted out. And that\u2019s it for the installation. The last step is to test the installation. Navigate to: >> cd ~/ParF/parflow/test {Or wherever you installed to} Then type: >> make check A lot of things will fly by as ParFlow verifies that it is working correctly. Most should pass, but some will likely fail if you have fewer processors than are being tested; for example, some of the tests need nine processors and these will fail on a system with only 4. You\u2019ll see a report at the end and as long as most pass, you\u2019re all done and ParFlow has been installed successfully. Step 7) Some troubleshooting tips The single most common reason steps of the installation don\u2019t go well is not being in the correct path. You can always verify your location in the directory tree using the \u201cpwd\u201d command and each step tells you where you should be before you begin entering commands. You mac is also case-sensitive so pay attention to capitalization. That also tends to be a wrench in the gears of the setup process. If you prefer to use the c-shell instead of the bash-shell most of the steps are the same but your environment variables are defined differently. Fortunately, the setup example in the ParFlow manual outlines the c-shell variations of all the steps we covered here. Lastly, if you have MacPorts or any other \u201chelper packages\u201d your system paths may have been altered from the defaults. Defining the environment variables at the bottom of your .bash_profile should take care of this. If you\u2019ve installed mpi properly but got errors with mpi, one solution can be turn off WIFI and keep WIFI off when running ParFLow. Step 8) What to do next See that wasn\u2019t nearly as bad as you thought it was going to be, so now what? Well, you could start getting to work with ParFlow (which is totally what you should do if you\u2019re a grad student), but maybe go outside, enjoy some fresh air, and see how things are today first. Maybe take a break and have a tasty beverage of your choosing while you ponder the intricacies of the scientific awesomeness you\u2019re going to unleash upon the world using ParFlow.","title":"Preliminaries"},{"location":"how_to_notes/HPC_systems/parflow_install/#macos-high-sierra","text":"Notes from Laura: 9/11/2018 Summary: Successful install: With GCC 8.1.0, MPI 3.1.2, Hypre 2.9.0b, Slio 4.10.2 Only modification from PF Blog is that I compiled MPI with clang per Nick\u2019s instructions (see the configure line below). Also it didn\u2019t work when I ran tests with WiFI on when I was at school (MPI error) but it worked once I turned it off. Edited the bash profile per the instructions in the Yosemite blog post: http://parflow.blogspot.com/2014/10/installing-parflow-in-os-x-yosemite.html Installed the latest GCC per blog instructions: http://hpc.sourceforge.net/ After this it still wasn\u2019t pointing to the correct gcc but this was fixed by a restart dhcp-10-134-192-214:~ laura$ gcc --version gcc (GCC) 8.1.0 Copyright (C) 2018 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. dhcp-10-134-192-214:~ laura$ gfortran --version GNU Fortran (GCC) 8.1.0 Copyright (C) 2018 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. Next I got the latest MPI: https://www.open-mpi.org/software/ompi/v3.1/ tar -xvf openmpi-3.1.2.tar.gz cd openmpi-3.1.2 ./configure --prefix=/usr/local/ CC=/usr/bin/clang CXX=/usr/bin/clang cd openmpi-3.1.2 sudo make all install ompi_info mpiexec \u2014version mpirun \u2014version Get Parflow: git clone https://github.com/parflow/parflow mv parflow/ parflow.git Get Silo: https://wci.llnl.gov/simulation/computer-codes/silo/downloads Follow instructions from ParFlow Blog to build: > cd silo > ./configure --disable-silex > make install > cd .. Get Hypre 2.9.0b: https://computation.llnl.gov/projects/hypre-scalable-linear-solvers-multigrid-methods/software Follow instructions from ParFlow Blog to build: > tar -xvf hypre-2.9.0b.tar.gz > mv hypre-2.9.0b hypre > cd hypre/src > ./configure --prefix=$HYPRE_DIR > make install Followed steps 7 and 8 from the blog to install par flow ( http://parflow.blogspot.com/2014/10/installing-parflow-in-os-x-yosemite.html ) All tests were failing with an MPI error when I had wifi turned on but when I turned it off they all pass Other People\u2019s Notes on High Seirra: From Nick: For MPI: Version 3.0.0 Two catches to this though. 1) I had to use clang, and 2) I specified the path and compilers manually. Here\u2019s the exact line I used:./configure --prefix=/usr/local/ CC=/usr/bin/clang CXX=/usr/bin/clang(FYI you can verify your MPI version with \u201compi_info\u201d at the command line and the version will be at the top) Next Hypre/Silo: It\u2019s Hypre 2.9.0b and Silo 2.9.1. I haven\u2019t tried any of the other versions lately, BUT these were installed with gcc, looks like v4.9.0 according to the config log And PF was built with the same GCC and gfortran version used for hypre. I had no luck getting PF to build with clang so, annoyingly, it\u2019s exactly the opposite of openMPI. From Reed: /Users/reed/parflow/libraries> gcc \u2014version gcc (GCC) 5.0.0 20141005 (experimental) Copyright (C) 2014 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. /Users/reed/parflow/libraries> gfortran \u2014version GNU Fortran (GCC) 5.0.0 20141005 (experimental) Copyright (C) 2014 Free Software Foundation, Inc. GNU Fortran comes with NO WARRANTY, to the extent permitted by law. You may redistribute copies of GNU Fortran under the terms of the GNU General Public License. For more information about these matters, see the file named COPYING","title":"MacOS High Sierra"},{"location":"how_to_notes/HPC_systems/parflow_install/#pfnetcdf-on-tacc-stampede2","text":"Katie Markovich, March 28, 2019 Home directory = /home1/06175/khm293 [make this your home directory!] .**bashrc** module load git module load gcc export CC=gcc export CXX=g++ export FC=gfortran export F77=gfortran export NCDIR=/home1/06175/khm293/ParF/netcdf4 export H5DIR=/home1/06175/khm293/ParF/hdf5 export MPI_DIR=/home1/06175/khm293/ParF/OMP-4.0.0 export MPI_RUN_DIR=/home1/06175/khm293/ParF/OMP-4.0.0/bin export HYPRE_DIR=/home1/06175/khm293/ParF/hypre export SILO_DIR=/home1/06175/khm293/ParF/silo export PARFLOW_DIR=/home1/06175/khm293/ParF/parflow PATH=$PATH:$PARFLOW_DIR/bin **hdf5** Download hdf5 here: https://www.hdfgroup.org/downloads/ >> cd ~ >> mkdir ParF >> cd ParF >>mkdir hdf5 >>exportH5DIR=${HOME}/ParF/hdf5 >>gunzip hdf5-1.10.4.tar.gz >>tar -xvf hdf5-1.10.4.tar >>cd hdf5-1.10.4/ >>CC=mpicc ./configure --enable-parallel --prefix=${H5DIR} >>make >>make install >>cd .. netCDF4 Download netCDF4 here: https://www.unidata.ucar.edu/downloads/netcdf/index.jsp >>mkdir ${HOME}/ParF/netcdf4 >>exportNCDIR=${HOME}/ParF/netcdf4 >>gunzip netcdf-4.6.2.tar.gz >>tar -xvf netcdf-4.6.2.tar >>cd netcdf-4.6.2/ >>CC=mpicc CPPFLAGS=-I${H5DIR}/include LDFLAGS=-L${H5DIR}/lib \\ ./configure --disable-shared--disable-dap--enable-parallel-tests --prefix=${NCDIR} >>make >>make install From Nick\u2019s post: openMPI >> cd ~ >> cd ParF >> mkdir OMPI-4.0.0 >> curl \"https://download.open-mpi.org/release/open-mpi/v4.0/openmpi-4.0.0.tar.gz\" -o \"openmpi-4.0.0.tar.gz\" >> gunzip -c openmpi-4.0.0.tar.gz | tar xf - >> cd openmpi-4.0.0 >> ./configure --prefix=$MPI_DIR >> make >> make install Silo >> cd ~/ParF >> curl \"https://wci.llnl.gov/content/assets/docs/simulation/computer-codes/silo/silo-4.10.2/silo-4.10.2.tar.gz\" -o \"silo-4.10.2.tar.gz\" >> gunzip -c silo-4.10.2.tar.gz | tar xf - >> mv silo-4.10.2 silo >> cd silo >> ./configure --disable-silex >> make install Hypre >> cd .. >> cd ~/ParF >> curl \"https://computation.llnl.gov/projects/hypre-scalable-linear-solvers-multigrid-methods/download/hypre-2.11.2.tar.gz\" -o \"hypre-2.11.2.tar.gz\" >> gunzip -c hypre-2.11.2.tar.gz | tar xf \u2013 >> mv hypre-2.11.2 hypre >> cd ~/ParF/ hypre/src >> ./configure --prefix=$HYPRE_DIR >> make install Parflow-NetCDF >> cd ~/ParF >> git clone https://github.com/parflow/parflow.git --branch master --single-branch >> mkdir build >> cd build >>cmake ../parflow -DCMAKE_INSTALL_PREFIX=$PARFLOW_DIR -DHYPRE_ROOT=$HYPRE_DIR -DPARFLOW_AMPS_LAYER=mpi1 -DPARFLOW_AMPS_SEQUENTIAL_IO=true -DPARFLOW_ENABLE_TIMING=true -DSILO_ROOT=$SILO_DIR -DPARFLOW_HAVE_CLM=ON -DNETCDF_DIR=$NCDIR -DHDF5_ROOT=$H5DIR","title":"PFNetCDF on TACC Stampede2"},{"location":"how_to_notes/HPC_systems/verde/","text":"Coming Soon!","title":"Verde"},{"location":"how_to_notes/data_and_file_transfers/apple_remote_desktop_ssh/","text":"Remote Desktop / SSH / Basic Command line Amanda Triplett 10/2019 Description: Accessing your computer via SSH through your command line with a general example of how to create a TAR file remotely and move it to your home computer. It also outlines how to remotely manage your desktop from mac to mac computer. Method 1a: From Apple to Apple: SSH Open a command line and type ssh \u201cyour_username\u201d@\u201dyour_static_IP\u201d If you\u2019ve never logged in you will have to type \u201cyes\u201d in order to connect Enter the password for the computer you\u2019re trying to access You are now accessing that other computer via the command line (anything you do while connected through this command line window will happen on the other computer) Method 1b: Creating a TAR file on a remote computer and pulling it to your current computer After you followed the steps in 1a, you can create a file and pull it, or just pull an already created file to your local computer Navigate to the folder where the file of interest is stored using a combination of ls (which tells you what is in the current folder and cd \u201cfile_dir\u201d to navigate to the new folder. Once you are in the folder where your file of interest is, type pwd to get the current file path If your file is small and does not need to be TARd skip the next step Creating a TAR Type the following: tar -czvf \u201cfile_name\u201d.tar.gz \u201cyour file path from pwd\u201d . C = create an archive Z = compress a folder with gzip (remove the z and .gz to do a regular tar) V = display progress in terminal (AKA verbose mode, this is also optional) F = (allows you to specify your filename) The \u201c .\u201d will stop the entire absolute file pathway from being there when you extract You should have a new tar file if you ls Transferring Open a new command window, this one will be accessing the computer you\u2019re on now Navigate (using cd and ls) to the folder where you want to put your file (you don\u2019t have to do this but I think it makes things easier) Once you\u2019re there, type the following \u201cscp \u201cyour_username\u201d@\u201dyour_static_ip\u201d:\u201dyour_file_path/filename.tar.gz\u201d . The \u201c .\u201d means put this file where I am right now (that\u2019s why we navigated to the right folder, otherwise you type the filepath of where you want the file to go instead) Change tar.gz to your appropriate file extension Unzipping the file (if needed) gunzip -c foo.tar.gz | tar xopf for tar.gz You don\u2019t need to specify a file path if you are already in the folder where you want things extracted tar -xvf filename.tar -C path you want to copy it to Method 2: From Apple to Apple: Using VPN and Remote Management Make sure your desktop has a static IP (all desktops have them at this time) Change your settings to allow remote management System Preferences -> sharing Check Screen sharing and remote management When you click remote management, select the capabilities you want when logging in remotely Get the VPN software Go to Vpn.arizona.edu NetID: your user name NetID Password: NetID Method: Push (If you\u2019re using Duo Mobile) After logging in click AnyConnect in the left toolbar Download the VPN software. Run the Cisco Anyconnect (you probably have to log in again) When the VPN runs the box may be blank. It should be: vpn.arizona.edu Go to Finder Select the Go menu from the top navigation bar (where the apple is) and go all the way to the bottom of the menu where it says connect to server Type in vnc://\u201dyour_static_IP\u201d and click connect You should now be able to control your desktop","title":"Remote Desktop / SSH / Basic Command line"},{"location":"how_to_notes/data_and_file_transfers/apple_remote_desktop_ssh/#remote-desktop-ssh-basic-command-line","text":"Amanda Triplett 10/2019 Description: Accessing your computer via SSH through your command line with a general example of how to create a TAR file remotely and move it to your home computer. It also outlines how to remotely manage your desktop from mac to mac computer.","title":"Remote Desktop / SSH / Basic Command line"},{"location":"how_to_notes/data_and_file_transfers/apple_remote_desktop_ssh/#method-1a-from-apple-to-apple-ssh","text":"Open a command line and type ssh \u201cyour_username\u201d@\u201dyour_static_IP\u201d If you\u2019ve never logged in you will have to type \u201cyes\u201d in order to connect Enter the password for the computer you\u2019re trying to access You are now accessing that other computer via the command line (anything you do while connected through this command line window will happen on the other computer)","title":"Method 1a: From Apple to Apple: SSH"},{"location":"how_to_notes/data_and_file_transfers/apple_remote_desktop_ssh/#method-1b-creating-a-tar-file-on-a-remote-computer-and-pulling-it-to-your-current-computer","text":"After you followed the steps in 1a, you can create a file and pull it, or just pull an already created file to your local computer Navigate to the folder where the file of interest is stored using a combination of ls (which tells you what is in the current folder and cd \u201cfile_dir\u201d to navigate to the new folder. Once you are in the folder where your file of interest is, type pwd to get the current file path If your file is small and does not need to be TARd skip the next step Creating a TAR Type the following: tar -czvf \u201cfile_name\u201d.tar.gz \u201cyour file path from pwd\u201d . C = create an archive Z = compress a folder with gzip (remove the z and .gz to do a regular tar) V = display progress in terminal (AKA verbose mode, this is also optional) F = (allows you to specify your filename) The \u201c .\u201d will stop the entire absolute file pathway from being there when you extract You should have a new tar file if you ls Transferring Open a new command window, this one will be accessing the computer you\u2019re on now Navigate (using cd and ls) to the folder where you want to put your file (you don\u2019t have to do this but I think it makes things easier) Once you\u2019re there, type the following \u201cscp \u201cyour_username\u201d@\u201dyour_static_ip\u201d:\u201dyour_file_path/filename.tar.gz\u201d . The \u201c .\u201d means put this file where I am right now (that\u2019s why we navigated to the right folder, otherwise you type the filepath of where you want the file to go instead) Change tar.gz to your appropriate file extension Unzipping the file (if needed) gunzip -c foo.tar.gz | tar xopf for tar.gz You don\u2019t need to specify a file path if you are already in the folder where you want things extracted tar -xvf filename.tar -C path you want to copy it to","title":"Method 1b: Creating a TAR file on a remote computer and pulling it to your current computer"},{"location":"how_to_notes/data_and_file_transfers/apple_remote_desktop_ssh/#method-2-from-apple-to-apple-using-vpn-and-remote-management","text":"Make sure your desktop has a static IP (all desktops have them at this time) Change your settings to allow remote management System Preferences -> sharing Check Screen sharing and remote management When you click remote management, select the capabilities you want when logging in remotely Get the VPN software Go to Vpn.arizona.edu NetID: your user name NetID Password: NetID Method: Push (If you\u2019re using Duo Mobile) After logging in click AnyConnect in the left toolbar Download the VPN software. Run the Cisco Anyconnect (you probably have to log in again) When the VPN runs the box may be blank. It should be: vpn.arizona.edu Go to Finder Select the Go menu from the top navigation bar (where the apple is) and go all the way to the bottom of the menu where it says connect to server Type in vnc://\u201dyour_static_IP\u201d and click connect You should now be able to control your desktop","title":"Method 2: From Apple to Apple: Using VPN and Remote Management"},{"location":"how_to_notes/data_and_file_transfers/cyverse/","text":"General Cyverse Information Laura Condon, Jan 2019 Description: Running notes with links and tips for doing file transfers with Cyverse. Feel free to update and add to this document as needed. Creating and Account and Getting Access: In order to upload data, you need to create a Cyverse account and be given permissions by Laura. Provide the username of your account and the e-mail you used to create the account (your UA e-mail) to Laura. Logging in: De.cyverse.org Our Server: You can find under Community Data/avra From command line: /iplant/home/shared/avra GUI file transferring with cyberduck https://cyberduck-quickstart.readthedocs.io/en/latest/# Setting up irods so you can do command line transfers: Sign up for a cyverse account: https://user.cyverse.org/register Install icommands so you can use irods Instructions for setting up on Mac: https://wiki.cyverse.org/wiki/display/DS/Setting+Up+iCommands#SettingUpiCommands-mac For setting up on Powell I used the page above and followed the link for the cent6 To check OS: cat /etc/*release Needed root access to install the irods commands so I logged in as root and then did: sudo yum install https://files.renci.org/pub/irods/releases/4.1.10/centos6/irods-icommands-4.1.10-centos6-x86_64.rpm Then login as your username and setup the cyverse connection according to step 3 Initialize irods by typing iinit and then putting in your user information following part two of the link on step 2 **Command line transfers with irods: ** First initialize irods like this: * _iinit_ * [data.cyverse.org](http://data.cyverse.org/) * Port: 1247 * User: yourusername * Zone: iplant * Then enter your cyverse password To upload: cd to the local directory with the data you want to transfer Then \u2018icd\u2019 to the location on Cyverse where you want to put the data icd /iplant/home/shared/avra #for avra icd /iplant/home/username #for your Cyverse home directory (note this is small and not where you should be storing things) iput -vP \u201cfilename\" To batch upload: iput -K -P -b -r -T folder-where-you-want-files-on-cyverse/ If you icd to where you want to put your files on cyverse then you simply need to put a period after the iput command\u2019 iput -K -P -b -r -T . -K \u201cThis causes iCommands to verify that transferred files weren't corrupted during the transfer by comparing the Checksums computed before and after the transfer.\u201d -P provides progress feedback -b allows for bulk uploads -r includes a folder or directory -T renews socket connection every 10 min If its a tar file you can extract it on Cyverse like this: ibun -x filename.tar new https://wiki.cyverse.org/wiki/display/DS/Dealing+with+Tar+and+other+Archive+Files **** You need to keep it to less than 2GB and several hundred files per Tar or the unzip ibun will fail. ** To Download: Basically the same thing but with iget instead of iput Links with irods info: icommands help: https://wiki.cyverse.org/wiki/display/DS/Using+iCommands Setting up irods commands for R: You should be able to run i commands in R using the system function. e.g. system(\u2018ils') If you get a warning saying command not found even though you have them installed you probably need to modify your R path so it can find them. To check this echo $PATH on command line and compare with system(\u2018echo $PATH) within R (You can also check this with Sys.getevn() to see everything or Sys.getenv(\"PATH\u201d) to just see the path in R) To fix you can set a new path appending the icommands to your current path like this: Sys.setenv(PATH='/usr/bin:/bin:/usr/sbin:/sbin:/usr/local/bin:/Applications/icommands') Then you also need to add the plugin: Sys.setenv(IRODS_PLUGINS_HOME=\"/Applications/icommands/plugins/\") Then check the path and check that it worked using getenv and testing an icommand. NOTE: If you get an irods host error at this point its probably because you haven\u2019t run iiint PERMANENT Fix: If you don\u2019t want to have to add the Sys.setenv to every script then do the following: vi ~/.Rprofile (just make the file if you don\u2019t have one) Add the following lines: Sys.setenv(PATH='/usr/bin:/bin:/usr/sbin:/sbin:/usr/local/bin:/Applications/icommands') Sys.setenv(IRODS_PLUGINS_HOME=\"/Applications/icommands/plugins/\") Note this will permanently set your environment variables Running an Analysis on Atmospheres Steps to run on the Cyverse Rstudio App Login: de.cyverse.org Click on Apps - Search for \u2018Rstudio\u2019 Select \u2018rocker-geospatial-3.5.0\u2019 # you can also add it to your favorites Give your analysis a name - you don\u2019t have to change anything though if you don\u2019t want Click \u2018Launch Analysis\u2019 Then if you click on \u2018Analyses\u2019 on the left you should see the job you just launched it will say \u2018running\u2019 but its probably still initializing. Click on the arrow next to its name to open the tab and you will see it launching your application. You will need to login to Cyverse and then R studio the first time it launches.For Rstudio the username and password are both rstudio Once you are in Rstudio - click on the terminal window iinit data.cyverse.org Port: 1247 User: lecondon Zone: iplant Checkout the GitHub repo git clone https://github.com/lecondon/CONUS1_Warming.git \\ Once you have done this you will see \u2018CONUS1_Warming\u2019 in your files tab Go to R scripts and click on the Rmd file you can run from here","title":"General Cyverse Information"},{"location":"how_to_notes/data_and_file_transfers/cyverse/#general-cyverse-information","text":"Laura Condon, Jan 2019","title":"General Cyverse Information"},{"location":"how_to_notes/data_and_file_transfers/cyverse/#description-running-notes-with-links-and-tips-for-doing-file-transfers-with-cyverse-feel-free-to-update-and-add-to-this-document-as-needed","text":"","title":"Description: Running notes with links and tips for doing file transfers with Cyverse. Feel free to update and add to this document as needed."},{"location":"how_to_notes/data_and_file_transfers/cyverse/#creating-and-account-and-getting-access","text":"In order to upload data, you need to create a Cyverse account and be given permissions by Laura. Provide the username of your account and the e-mail you used to create the account (your UA e-mail) to Laura. Logging in: De.cyverse.org Our Server: You can find under Community Data/avra From command line: /iplant/home/shared/avra GUI file transferring with cyberduck https://cyberduck-quickstart.readthedocs.io/en/latest/#","title":"Creating and Account and Getting Access:"},{"location":"how_to_notes/data_and_file_transfers/cyverse/#setting-up-irods-so-you-can-do-command-line-transfers","text":"Sign up for a cyverse account: https://user.cyverse.org/register Install icommands so you can use irods Instructions for setting up on Mac: https://wiki.cyverse.org/wiki/display/DS/Setting+Up+iCommands#SettingUpiCommands-mac For setting up on Powell I used the page above and followed the link for the cent6 To check OS: cat /etc/*release Needed root access to install the irods commands so I logged in as root and then did: sudo yum install https://files.renci.org/pub/irods/releases/4.1.10/centos6/irods-icommands-4.1.10-centos6-x86_64.rpm Then login as your username and setup the cyverse connection according to step 3 Initialize irods by typing iinit and then putting in your user information following part two of the link on step 2 **Command line transfers with irods: ** First initialize irods like this: * _iinit_ * [data.cyverse.org](http://data.cyverse.org/) * Port: 1247 * User: yourusername * Zone: iplant * Then enter your cyverse password To upload: cd to the local directory with the data you want to transfer Then \u2018icd\u2019 to the location on Cyverse where you want to put the data icd /iplant/home/shared/avra #for avra icd /iplant/home/username #for your Cyverse home directory (note this is small and not where you should be storing things) iput -vP \u201cfilename\" To batch upload: iput -K -P -b -r -T folder-where-you-want-files-on-cyverse/ If you icd to where you want to put your files on cyverse then you simply need to put a period after the iput command\u2019 iput -K -P -b -r -T . -K \u201cThis causes iCommands to verify that transferred files weren't corrupted during the transfer by comparing the Checksums computed before and after the transfer.\u201d -P provides progress feedback -b allows for bulk uploads -r includes a folder or directory -T renews socket connection every 10 min If its a tar file you can extract it on Cyverse like this: ibun -x filename.tar new https://wiki.cyverse.org/wiki/display/DS/Dealing+with+Tar+and+other+Archive+Files **** You need to keep it to less than 2GB and several hundred files per Tar or the unzip ibun will fail. ** To Download: Basically the same thing but with iget instead of iput Links with irods info: icommands help: https://wiki.cyverse.org/wiki/display/DS/Using+iCommands Setting up irods commands for R: You should be able to run i commands in R using the system function. e.g. system(\u2018ils') If you get a warning saying command not found even though you have them installed you probably need to modify your R path so it can find them. To check this echo $PATH on command line and compare with system(\u2018echo $PATH) within R (You can also check this with Sys.getevn() to see everything or Sys.getenv(\"PATH\u201d) to just see the path in R) To fix you can set a new path appending the icommands to your current path like this: Sys.setenv(PATH='/usr/bin:/bin:/usr/sbin:/sbin:/usr/local/bin:/Applications/icommands') Then you also need to add the plugin: Sys.setenv(IRODS_PLUGINS_HOME=\"/Applications/icommands/plugins/\") Then check the path and check that it worked using getenv and testing an icommand. NOTE: If you get an irods host error at this point its probably because you haven\u2019t run iiint PERMANENT Fix: If you don\u2019t want to have to add the Sys.setenv to every script then do the following: vi ~/.Rprofile (just make the file if you don\u2019t have one) Add the following lines: Sys.setenv(PATH='/usr/bin:/bin:/usr/sbin:/sbin:/usr/local/bin:/Applications/icommands') Sys.setenv(IRODS_PLUGINS_HOME=\"/Applications/icommands/plugins/\") Note this will permanently set your environment variables Running an Analysis on Atmospheres Steps to run on the Cyverse Rstudio App Login: de.cyverse.org Click on Apps - Search for \u2018Rstudio\u2019 Select \u2018rocker-geospatial-3.5.0\u2019 # you can also add it to your favorites Give your analysis a name - you don\u2019t have to change anything though if you don\u2019t want Click \u2018Launch Analysis\u2019 Then if you click on \u2018Analyses\u2019 on the left you should see the job you just launched it will say \u2018running\u2019 but its probably still initializing. Click on the arrow next to its name to open the tab and you will see it launching your application. You will need to login to Cyverse and then R studio the first time it launches.For Rstudio the username and password are both rstudio Once you are in Rstudio - click on the terminal window iinit data.cyverse.org Port: 1247 User: lecondon Zone: iplant Checkout the GitHub repo git clone https://github.com/lecondon/CONUS1_Warming.git \\ Once you have done this you will see \u2018CONUS1_Warming\u2019 in your files tab Go to R scripts and click on the Rmd file you can run from here","title":"Setting up irods so you can do command line transfers:"},{"location":"how_to_notes/data_and_file_transfers/globus/","text":"Globus File Transfer Abe, May 8th, 2020 Description Setting up and using globus to transfer files between a local machine and server (uahpc) Software: Globus, UAHPC Datasets: N/A Links: https://www.globus.org/ Globus provides a simple way to transfer files between a local device and a server account instead of using scp or another file transfer method. It is especially useful for large sets of files. Setting up Account: A good reference: https://www.osc.edu/book/export/html/3578 To Login: Visit https://www.globus.org/ , go to login and select \u201cThe University of Arizona\u201d You will be prompted to login with your UA NetID Set-up Endpoints: Endpoints are essentially paths that are linked and configured for streamlined file transfer. Personal endpoints can be setup for personal systems: research desktop or personal laptop When logged-in, click on \u201cendpoints\u201d on the left panel At the top select \u201ccreate personal endpoint\u201d Download Globus Connect Personal (follow the install instructions) You can select the default home directory or specify a different directory for the endpoint if you wish You will also name the endpoint (ex: \u201cAbe\u2019s Research Desktop\u201d) You should now have a personal endpoint for the device which you are currently working on Transfering Files To transfer files between this personal endpoint and the UAHPC server, you will have to search for it on the globus website: When logged-in, click on \u201cfile manager\u201d on the left panel The left panel is where you want to transfer from...so to transfer something from your desktop click in the \u201ccollections\u201d search bar and select the personal endpoint that you just created (for ex: \u201cAbe\u2019s Research Desktop\u201d) From here you can navigate the system for the directory or files that you wish to transfer On the right window panel is where you will select the server endpoint (or another personal endpoint). For UAHPC you search UA HPC Filesystems More info found here: https://public.confluence.arizona.edu/display/UAHPC/Transferring+Files Once in the UA server you can navigate your UAHPC directories and go to where you want to transfer the files. (I had to physically type in the directory I wanted on the HPC) Click \u201cStart\u201d once the correct file and directories are selected. Note you can transfer to/from either endpoint You\u2019ll get a confirmation email once it was successfully transferred Other links and resources: Install guide of Mac: https://docs.globus.org/how-to/globus-connect-personal-mac/ Penn State Explanation: https://www.youtube.com/watch?v=iIfeVxplZ8U Extra Information for NCAR HPC (Cheyenne): This link has helpful resources: https://www2.cisl.ucar.edu/resources/storage-and-file-systems/globus-file-transfers Log in to your Globus account Go to File manager Go to panel view so you can see the source and destination of your files Type NCAR GLADE into the collection for either source or destination, select your personal endpoint for the other You will be asked to log in, use your normal log in credentials when you are accessing NCAR resources through terminal Click advanced and for certificate length type \u2018720\u2019 this will allow you to extend the credential to 30 days as opposed to the usual 24 You will authenticate your log in whatever way you have it set up, likely Duo Go to /glade/scratch/\u201dyour_username\u201d and you should see the file tree as you have it set up Start transfer! Information for Google Drive transfers: Follow the steps in the link below to setup your Google Drive account as an endpoint. https://public.confluence.arizona.edu/display/UAHPC/Transferring+Files#TransferringFiles-GoogleDrive","title":"Globus File Transfer"},{"location":"how_to_notes/data_and_file_transfers/globus/#globus-file-transfer","text":"Abe, May 8th, 2020","title":"Globus File Transfer"},{"location":"how_to_notes/data_and_file_transfers/globus/#description","text":"Setting up and using globus to transfer files between a local machine and server (uahpc) Software: Globus, UAHPC Datasets: N/A Links: https://www.globus.org/ Globus provides a simple way to transfer files between a local device and a server account instead of using scp or another file transfer method. It is especially useful for large sets of files.","title":"Description"},{"location":"how_to_notes/data_and_file_transfers/globus/#setting-up-account","text":"A good reference: https://www.osc.edu/book/export/html/3578 To Login: Visit https://www.globus.org/ , go to login and select \u201cThe University of Arizona\u201d You will be prompted to login with your UA NetID","title":"Setting up Account:"},{"location":"how_to_notes/data_and_file_transfers/globus/#set-up-endpoints","text":"Endpoints are essentially paths that are linked and configured for streamlined file transfer. Personal endpoints can be setup for personal systems: research desktop or personal laptop When logged-in, click on \u201cendpoints\u201d on the left panel At the top select \u201ccreate personal endpoint\u201d Download Globus Connect Personal (follow the install instructions) You can select the default home directory or specify a different directory for the endpoint if you wish You will also name the endpoint (ex: \u201cAbe\u2019s Research Desktop\u201d) You should now have a personal endpoint for the device which you are currently working on","title":"Set-up Endpoints:"},{"location":"how_to_notes/data_and_file_transfers/globus/#transfering-files","text":"To transfer files between this personal endpoint and the UAHPC server, you will have to search for it on the globus website: When logged-in, click on \u201cfile manager\u201d on the left panel The left panel is where you want to transfer from...so to transfer something from your desktop click in the \u201ccollections\u201d search bar and select the personal endpoint that you just created (for ex: \u201cAbe\u2019s Research Desktop\u201d) From here you can navigate the system for the directory or files that you wish to transfer On the right window panel is where you will select the server endpoint (or another personal endpoint). For UAHPC you search UA HPC Filesystems More info found here: https://public.confluence.arizona.edu/display/UAHPC/Transferring+Files Once in the UA server you can navigate your UAHPC directories and go to where you want to transfer the files. (I had to physically type in the directory I wanted on the HPC) Click \u201cStart\u201d once the correct file and directories are selected. Note you can transfer to/from either endpoint You\u2019ll get a confirmation email once it was successfully transferred Other links and resources: Install guide of Mac: https://docs.globus.org/how-to/globus-connect-personal-mac/ Penn State Explanation: https://www.youtube.com/watch?v=iIfeVxplZ8U","title":"Transfering Files"},{"location":"how_to_notes/data_and_file_transfers/globus/#extra-information-for-ncar-hpc-cheyenne","text":"This link has helpful resources: https://www2.cisl.ucar.edu/resources/storage-and-file-systems/globus-file-transfers Log in to your Globus account Go to File manager Go to panel view so you can see the source and destination of your files Type NCAR GLADE into the collection for either source or destination, select your personal endpoint for the other You will be asked to log in, use your normal log in credentials when you are accessing NCAR resources through terminal Click advanced and for certificate length type \u2018720\u2019 this will allow you to extend the credential to 30 days as opposed to the usual 24 You will authenticate your log in whatever way you have it set up, likely Duo Go to /glade/scratch/\u201dyour_username\u201d and you should see the file tree as you have it set up Start transfer!","title":"Extra Information for NCAR HPC (Cheyenne):"},{"location":"how_to_notes/data_and_file_transfers/globus/#information-for-google-drive-transfers","text":"Follow the steps in the link below to setup your Google Drive account as an endpoint. https://public.confluence.arizona.edu/display/UAHPC/Transferring+Files#TransferringFiles-GoogleDrive","title":"Information for Google Drive transfers:"},{"location":"useful_external_resources/example_scripts/","text":"Group Example Scripts Jupyter Notebook showing the storage-elevation changes for Lake Powell using USGS and ResOps data Jupyter Notebook showing how to interact with Google drive via a notebook and a new way to analyze Parflow outputs using xarray","title":"Group Example Scripts"},{"location":"useful_external_resources/example_scripts/#group-example-scripts","text":"Jupyter Notebook showing the storage-elevation changes for Lake Powell using USGS and ResOps data Jupyter Notebook showing how to interact with Google drive via a notebook and a new way to analyze Parflow outputs using xarray","title":"Group Example Scripts"},{"location":"useful_external_resources/great_python_packages/","text":"","title":"Great python packages"},{"location":"useful_external_resources/parflow_resources/","text":"","title":"Parflow resources"},{"location":"useful_external_resources/research_repos/","text":"","title":"Research repos"}]}